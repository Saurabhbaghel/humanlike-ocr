{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil, random\n",
    "import numpy as np\n",
    "import torch\n",
    "import cv2\n",
    "\n",
    "from torchmetrics.classification import AveragePrecision, MulticlassAccuracy, MulticlassF1Score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import torchvision as tv\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from ntm.encapsulated import EncapsulatedNTM\n",
    "from config import configuration as configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, input_channels: int, out_channels: int, kernel_size: tuple = (3, 3)):\n",
    "        \"\"\"Basic Conv Block with convolution layer, etc.\n",
    "\n",
    "        Args:\n",
    "            input_channels (int): _description_\n",
    "            out_channels (int): _description_\n",
    "            kernel_size (tuple, optional): _description_. Defaults to (3, 3).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_channels = input_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        \n",
    "        self.conv = nn.Conv2d(self.input_channels, self.out_channels, self.kernel_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(self.kernel_size)\n",
    "        self.dropout = nn.Dropout2d()\n",
    "        self.batchnorm = nn.BatchNorm2d(self.out_channels)\n",
    "    \n",
    "    def forward(self, x, training:bool = True):\n",
    "        conv_res = self.conv(x)\n",
    "        activated = self.relu(conv_res)\n",
    "        pooled = self.maxpool(activated)\n",
    "        if training:\n",
    "            pooled = self.dropout(pooled)\n",
    "        y = self.batchnorm(pooled)\n",
    "        return y\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"ConvBlock({self.input_channels, self.out_channels, self.kernel_size})\"\n",
    "\n",
    "\n",
    "class FeedforwardController(nn.Module):\n",
    "    def __init__(self, num_inputs:int, num_layers:int) -> None:\n",
    "        super().__init__()\n",
    "        self.num_inputs = num_inputs\n",
    "        # self.num_outputs = num_outputs\n",
    "        self.num_layers = num_layers\n",
    "        # self.batch_size = batch_size\n",
    "        self.device_ = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.model_ = nn.Sequential(\n",
    "            ConvBlock(self.num_inputs, 32, (2,2)),\n",
    "            ConvBlock(32, 64, (2,2))\n",
    "        )\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.final_layer = nn.LazyLinear(44)\n",
    "\n",
    "    def forward(self, x, training:bool=True):\n",
    "        y = self.model_(x)\n",
    "        y = self.flatten(y)\n",
    "        y = self.final_layer(y)\n",
    "        return F.softmax(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\work\\envs_win\\torch\\lib\\site-packages\\torch\\nn\\modules\\lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    }
   ],
   "source": [
    "num_inputs = 1\n",
    "num_outputs = 44\n",
    "controller_size = 3136 #1024     3136 for FeedforwardController in the cell above 12800 for feedforwardcontroller with \n",
    "controller_layers = 1\n",
    "num_heads = 4\n",
    "num_classes = 44\n",
    "N, M = 10, 10\n",
    "controller_ = FeedforwardController\n",
    "num_epochs = 5\n",
    "\n",
    "batch_size = 2 #220\n",
    "\n",
    "# defining the network\n",
    "# net = EncapsulatedNTM(\n",
    "#     num_inputs,\n",
    "#     num_outputs,\n",
    "#     controller_size,\n",
    "#     controller_layers,\n",
    "#     num_heads,\n",
    "#     N,\n",
    "#     M,\n",
    "#     controller_= controller_, #FeedforwardController,\n",
    "#     vanilla_heads=True\n",
    "# )\n",
    "net = FeedforwardController(1, 3)\n",
    "device_ = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, annotations_file, img_dir):\n",
    "        self.images_csv = annotations_file.reset_index(drop=True) #pd.read_csv(annotations_file).reset_index(drop=True)\n",
    "        self.img_dir = img_dir\n",
    "        # self.transforms_ = tv.transforms.Compose([\n",
    "        #     tv.transforms.Resize(40),\n",
    "        #     # tv.transforms.CenterCrop(40),\n",
    "        #     tv.transforms.ConvertImageDtype(torch.float),\n",
    "        #     tv.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        # ])\n",
    "        \n",
    "    def transforms_(self, image):\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) if image.ndim == 3 else image\n",
    "        thresh = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY_INV, 21, 10)\n",
    "        # normed= cv2.normalize(thresh, None, 0, 1.0, cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
    "        skeleton = cv2.ximgproc.thinning(thresh, None, 1)\n",
    "        image = cv2.resize(skeleton, (32, 32))\n",
    "        return image\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images_csv)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path = os.path.join(self.img_dir, self.images_csv.iloc[index, 0])\n",
    "        # image = tv.io.image.read_image(img_path)\n",
    "        image = cv2.imread(img_path, cv2.IMREAD_UNCHANGED)\n",
    "        image_ = self.transforms_(image) #.to(device_)\n",
    "        image = torch.from_numpy(image_)\n",
    "        label = self.images_csv.iloc[index, 1]\n",
    "        label = torch.nn.functional.one_hot(torch.tensor(label).to(torch.int64), num_classes=num_outputs)\n",
    "        return image.unsqueeze(0), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "Config = configs()\n",
    "\n",
    "PATH_PRINTED_TRAIN_CSV, PATH_PRINTED_TRAIN_IMGS = Config.paths(printed=True, train=True)\n",
    "PATH_PRINTED_VAL_CSV, PATH_PRINTED_VAL_IMGS = Config.paths(printed=True, train=False)\n",
    "\n",
    "PATH_HW_TRAIN_CSV, PATH_HW_TRAIN_IMGS = Config.paths(printed=False, train=True)\n",
    "PATH_HW_VAL_CSV, PATH_HW_VAL_IMGS = Config.paths(printed=False, train=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset(pd.read_csv(PATH_PRINTED_TRAIN_CSV), PATH_PRINTED_TRAIN_IMGS)\n",
    "val_dataset = dataset(pd.read_csv(PATH_PRINTED_VAL_CSV), PATH_PRINTED_VAL_IMGS)\n",
    "\n",
    "# creating dataloader\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss\n",
    "loss_fn = torch.nn.BCELoss()\n",
    "\n",
    "# optimizer\n",
    "optimizer = torch.optim.Adam(net.parameters())\n",
    "\n",
    "# metric\n",
    "# metric = AveragePrecision(task=\"multiclass\", num_classes=num_outputs)\n",
    "metric = MulticlassAccuracy(num_classes=44)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saura\\AppData\\Local\\Temp\\ipykernel_12408\\4212073385.py:55: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.softmax(y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 0.002, train acc 0.994\n",
      "Loss train 0.002,  validation 0.002,  val acc 0.986\n",
      "Epoch 1\n",
      "epoch 1, loss 0.000, train acc 0.991\n",
      "Loss train 0.000,  validation 0.002,  val acc 0.988\n",
      "Epoch 2\n",
      "epoch 2, loss 0.010, train acc 0.991\n",
      "Loss train 0.010,  validation 0.001,  val acc 0.991\n",
      "Epoch 3\n",
      "epoch 3, loss 0.000, train acc 0.992\n",
      "Loss train 0.000,  validation 0.001,  val acc 0.993\n",
      "Epoch 4\n",
      "epoch 4, loss 0.000, train acc 0.991\n",
      "Loss train 0.000,  validation 0.002,  val acc 0.987\n",
      "Epoch 5\n",
      "epoch 5, loss 0.000, train acc 0.992\n",
      "Loss train 0.000,  validation 0.001,  val acc 0.993\n",
      "Epoch 6\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 31\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[39m# net.init_sequence(batch_size)\u001b[39;00m\n\u001b[0;32m     30\u001b[0m training \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m outputs \u001b[39m=\u001b[39m net(inputs\u001b[39m.\u001b[39;49mtype(torch\u001b[39m.\u001b[39;49mfloat))\n\u001b[0;32m     32\u001b[0m avg_loss \u001b[39m=\u001b[39m loss_fn(outputs, labels)\n\u001b[0;32m     34\u001b[0m list_outputs\u001b[39m.\u001b[39mappend(outputs)\n",
      "File \u001b[1;32md:\\work\\envs_win\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[3], line 52\u001b[0m, in \u001b[0;36mFeedforwardController.forward\u001b[1;34m(self, x, training)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, training:\u001b[39mbool\u001b[39m\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m---> 52\u001b[0m     y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel_(x)\n\u001b[0;32m     53\u001b[0m     y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mflatten(y)\n\u001b[0;32m     54\u001b[0m     y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfinal_layer(y)\n",
      "File \u001b[1;32md:\\work\\envs_win\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\work\\envs_win\\torch\\lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32md:\\work\\envs_win\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[3], line 23\u001b[0m, in \u001b[0;36mConvBlock.forward\u001b[1;34m(self, x, training)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, training:\u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m---> 23\u001b[0m     conv_res \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv(x)\n\u001b[0;32m     24\u001b[0m     activated \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(conv_res)\n\u001b[0;32m     25\u001b[0m     pooled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmaxpool(activated)\n",
      "File \u001b[1;32md:\\work\\envs_win\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\work\\envs_win\\torch\\lib\\site-packages\\torch\\nn\\modules\\conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[1;32md:\\work\\envs_win\\torch\\lib\\site-packages\\torch\\nn\\modules\\conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[0;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[0;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[1;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[0;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 50\n",
    "\n",
    "\n",
    "# training loop\n",
    "cnn_train_loss = []\n",
    "cnn_train_acc = []\n",
    "cnn_val_loss = []\n",
    "cnn_val_acc = []\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(\"Epoch {}\".format(epoch))\n",
    "\n",
    "    last_loss = 0.0\n",
    "    list_outputs = []\n",
    "    list_labels = []\n",
    "    \n",
    "    net.train()\n",
    "    for i, data in enumerate(train_dataloader):\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        inputs, labels = data[0].to(device_), data[1].to(device_)\n",
    "        # print(inputs.size())\n",
    "        labels = labels.type(torch.float)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # net.init_sequence(batch_size)\n",
    "\n",
    "\n",
    "        training = True\n",
    "        outputs = net(inputs.type(torch.float))\n",
    "        avg_loss = loss_fn(outputs, labels)\n",
    "\n",
    "        list_outputs.append(outputs)\n",
    "        list_labels.append(torch.argmax(labels, dim=1))\n",
    "        \n",
    "        # avg_prec = metric(outputs, torch.argmax(labels, dim=1))\n",
    "        # print(torch.argmax(outputs, dim=1), torch.argmax(labels, dim=1))\n",
    "        avg_loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += avg_loss.item()\n",
    "        last_loss = running_loss\n",
    "        # last_avg_prec = avg_prec\n",
    "\n",
    "        # appending the train loss for grpah plotting\n",
    "        cnn_train_loss.append(avg_loss.item())\n",
    "\n",
    "    outputs = torch.cat(list_outputs, dim=0)\n",
    "    labels = torch.cat(list_labels).squeeze()\n",
    "    # print(labels)\n",
    "    # print(torch.argmax(outputs, dim=1))\n",
    "    acc = metric(outputs, labels)\n",
    "\n",
    "    # adding for the graph \n",
    "    cnn_train_acc.append(acc)\n",
    "\n",
    "    print(\"epoch {}, loss {:.3f}, train acc {:.3f}\".format(epoch, last_loss, acc))\n",
    "    # model_scripted = torch.jit.script(net)\n",
    "    # torch.save(net.state_dict(), \"model_state_convnet_01-06-23_.pt\")\n",
    "    # torch.save(net, \"model_convnet_{}_{}.pt\".format(\"12-05-23\", epoch))\n",
    "    \n",
    "    # validation\n",
    "    list_outputs = []\n",
    "    list_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        running_vloss = 0.0\n",
    "        for i, data in enumerate(val_dataloader):\n",
    "            # net.init_sequence(batch_size)\n",
    "\n",
    "            vinputs, vlabels = data[0].to(device_), data[1].to(device_)\n",
    "            vlabels = vlabels.type(torch.float)\n",
    "            voutputs = net(vinputs.type(torch.float))\n",
    "            list_outputs.append(voutputs)\n",
    "            list_labels.append(torch.argmax(vlabels, dim=1))\n",
    "            vloss = loss_fn(voutputs, vlabels)\n",
    "            running_vloss += vloss\n",
    "\n",
    "            # appending the val loss for the graph plotting\n",
    "            cnn_val_loss.append(vloss.item())\n",
    "\n",
    "        voutputs = torch.cat(list_outputs, dim=0)\n",
    "        vlabels = torch.cat(list_labels).squeeze()\n",
    "        vacc = metric(voutputs, vlabels)\n",
    "        \n",
    "        # appending the val accuracy for plotting graph\n",
    "        cnn_val_acc.append(vacc)\n",
    "\n",
    "        avg_vloss = running_vloss / (i+1)\n",
    "        print(\"Loss train {:.3f},  validation {:.3f},  val acc {:.3f}\".format(avg_loss, avg_vloss, vacc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset(pd.read_csv(PATH_HW_TRAIN_CSV), PATH_HW_TRAIN_IMGS)\n",
    "val_dataset = dataset(pd.read_csv(PATH_HW_VAL_CSV), PATH_HW_VAL_IMGS)\n",
    "\n",
    "# creating dataloader\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saura\\AppData\\Local\\Temp\\ipykernel_12408\\4212073385.py:55: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.softmax(y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 3.443, train acc 0.051\n",
      "Loss train 3.443,  validation 2.539,  val acc 0.026\n",
      "Epoch 1\n",
      "epoch 1, loss 4.545, train acc 0.034\n",
      "Loss train 4.545,  validation 4.400,  val acc 0.023\n",
      "Epoch 2\n",
      "epoch 2, loss 4.545, train acc 0.023\n",
      "Loss train 4.545,  validation 4.400,  val acc 0.023\n",
      "Epoch 3\n",
      "epoch 3, loss 4.545, train acc 0.023\n",
      "Loss train 4.545,  validation 4.399,  val acc 0.023\n",
      "Epoch 4\n",
      "epoch 4, loss 4.545, train acc 0.023\n",
      "Loss train 4.545,  validation 4.399,  val acc 0.023\n",
      "Epoch 5\n",
      "epoch 5, loss 4.545, train acc 0.023\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 70\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m     69\u001b[0m     running_vloss \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mfor\u001b[39;00m i, data \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(val_dataloader):\n\u001b[0;32m     71\u001b[0m         \u001b[39m# net.init_sequence(batch_size)\u001b[39;00m\n\u001b[0;32m     73\u001b[0m         vinputs, vlabels \u001b[39m=\u001b[39m data[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mto(device_), data[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mto(device_)\n\u001b[0;32m     74\u001b[0m         vlabels \u001b[39m=\u001b[39m vlabels\u001b[39m.\u001b[39mtype(torch\u001b[39m.\u001b[39mfloat)\n",
      "File \u001b[1;32md:\\work\\envs_win\\torch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    631\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    632\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    633\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 634\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    635\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    636\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    638\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32md:\\work\\envs_win\\torch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    676\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    677\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 678\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    679\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    680\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32md:\\work\\envs_win\\torch\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32md:\\work\\envs_win\\torch\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[14], line 30\u001b[0m, in \u001b[0;36mdataset.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     28\u001b[0m image \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfrom_numpy(image_)\n\u001b[0;32m     29\u001b[0m label \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimages_csv\u001b[39m.\u001b[39miloc[index, \u001b[39m1\u001b[39m]\n\u001b[1;32m---> 30\u001b[0m label \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mnn\u001b[39m.\u001b[39;49mfunctional\u001b[39m.\u001b[39;49mone_hot(torch\u001b[39m.\u001b[39;49mtensor(label)\u001b[39m.\u001b[39;49mto(torch\u001b[39m.\u001b[39;49mint64), num_classes\u001b[39m=\u001b[39;49mnum_outputs)\n\u001b[0;32m     31\u001b[0m \u001b[39mreturn\u001b[39;00m image\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m), label\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 50\n",
    "\n",
    "\n",
    "# training loop\n",
    "cnn_train_loss = []\n",
    "cnn_train_acc = []\n",
    "cnn_val_loss = []\n",
    "cnn_val_acc = []\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(\"Epoch {}\".format(epoch))\n",
    "\n",
    "    last_loss = 0.0\n",
    "    list_outputs = []\n",
    "    list_labels = []\n",
    "    \n",
    "    net.train()\n",
    "    for i, data in enumerate(train_dataloader):\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        inputs, labels = data[0].to(device_), data[1].to(device_)\n",
    "        # print(inputs.size())\n",
    "        labels = labels.type(torch.float)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # net.init_sequence(batch_size)\n",
    "\n",
    "\n",
    "        training = True\n",
    "        outputs = net(inputs.type(torch.float))\n",
    "        avg_loss = loss_fn(outputs, labels)\n",
    "\n",
    "        list_outputs.append(outputs)\n",
    "        list_labels.append(torch.argmax(labels, dim=1))\n",
    "        \n",
    "        # avg_prec = metric(outputs, torch.argmax(labels, dim=1))\n",
    "        # print(torch.argmax(outputs, dim=1), torch.argmax(labels, dim=1))\n",
    "        avg_loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += avg_loss.item()\n",
    "        last_loss = running_loss\n",
    "        # last_avg_prec = avg_prec\n",
    "\n",
    "        # appending the train loss for grpah plotting\n",
    "        cnn_train_loss.append(avg_loss.item())\n",
    "\n",
    "    outputs = torch.cat(list_outputs, dim=0)\n",
    "    labels = torch.cat(list_labels).squeeze()\n",
    "    # print(labels)\n",
    "    # print(torch.argmax(outputs, dim=1))\n",
    "    acc = metric(outputs, labels)\n",
    "\n",
    "    # adding for the graph \n",
    "    cnn_train_acc.append(acc)\n",
    "\n",
    "    print(\"epoch {}, loss {:.3f}, train acc {:.3f}\".format(epoch, last_loss, acc))\n",
    "    # model_scripted = torch.jit.script(net)\n",
    "    # torch.save(net.state_dict(), \"model_state_convnet_01-06-23_.pt\")\n",
    "    # torch.save(net, \"model_convnet_{}_{}.pt\".format(\"12-05-23\", epoch))\n",
    "    \n",
    "    # validation\n",
    "    list_outputs = []\n",
    "    list_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        running_vloss = 0.0\n",
    "        for i, data in enumerate(val_dataloader):\n",
    "            # net.init_sequence(batch_size)\n",
    "\n",
    "            vinputs, vlabels = data[0].to(device_), data[1].to(device_)\n",
    "            vlabels = vlabels.type(torch.float)\n",
    "            voutputs = net(vinputs.type(torch.float))\n",
    "            list_outputs.append(voutputs)\n",
    "            list_labels.append(torch.argmax(vlabels, dim=1))\n",
    "            vloss = loss_fn(voutputs, vlabels)\n",
    "            running_vloss += vloss\n",
    "\n",
    "            # appending the val loss for the graph plotting\n",
    "            cnn_val_loss.append(vloss.item())\n",
    "\n",
    "        voutputs = torch.cat(list_outputs, dim=0)\n",
    "        vlabels = torch.cat(list_labels).squeeze()\n",
    "        vacc = metric(voutputs, vlabels)\n",
    "        \n",
    "        # appending the val accuracy for plotting graph\n",
    "        cnn_val_acc.append(vacc)\n",
    "\n",
    "        avg_vloss = running_vloss / (i+1)\n",
    "        print(\"Loss train {:.3f},  validation {:.3f},  val acc {:.3f}\".format(avg_loss, avg_vloss, vacc))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, input_channels: int, out_channels: int, kernel_size: tuple = (3, 3)):\n",
    "        \"\"\"Basic Conv Block with convolution layer, etc.\n",
    "\n",
    "        Args:\n",
    "            input_channels (int): _description_\n",
    "            out_channels (int): _description_\n",
    "            kernel_size (tuple, optional): _description_. Defaults to (3, 3).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_channels = input_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        \n",
    "        self.conv = nn.Conv2d(self.input_channels, self.out_channels, self.kernel_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(self.kernel_size)\n",
    "        self.dropout = nn.Dropout2d()\n",
    "        self.batchnorm = nn.BatchNorm2d(self.out_channels)\n",
    "    \n",
    "    def forward(self, x, training:bool = True):\n",
    "        conv_res = self.conv(x)\n",
    "        activated = self.relu(conv_res)\n",
    "        pooled = self.maxpool(activated)\n",
    "        if training:\n",
    "            pooled = self.dropout(pooled)\n",
    "        y = self.batchnorm(pooled)\n",
    "        return y\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"ConvBlock({self.input_channels, self.out_channels, self.kernel_size})\"\n",
    "\n",
    "\n",
    "class FeedforwardController(nn.Module):\n",
    "    def __init__(self, num_inputs:int, num_layers:int) -> None:\n",
    "        super().__init__()\n",
    "        self.num_inputs = num_inputs\n",
    "        # self.num_outputs = num_outputs\n",
    "        self.num_layers = num_layers\n",
    "        # self.batch_size = batch_size\n",
    "        self.device_ = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.model_ = nn.Sequential(\n",
    "            ConvBlock(self.num_inputs, 32, (2,2)),\n",
    "            ConvBlock(32, 64, (2,2))\n",
    "        )\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.final_layer = nn.LazyLinear(44)\n",
    "\n",
    "    def forward(self, x, training:bool=True):\n",
    "        y = self.model_(x)\n",
    "        y = self.flatten(y)\n",
    "        # y = self.final_layer(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\work\\envs_win\\torch\\lib\\site-packages\\torch\\nn\\modules\\lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    }
   ],
   "source": [
    "num_inputs = 1\n",
    "num_outputs = 44\n",
    "controller_size = 3136 #1024     3136 for FeedforwardController in the cell above 12800 for feedforwardcontroller with \n",
    "controller_layers = 1\n",
    "num_heads = 4\n",
    "num_classes = 44\n",
    "N, M = 10, 10\n",
    "controller_ = FeedforwardController\n",
    "num_epochs = 5\n",
    "\n",
    "batch_size = 2 #220\n",
    "\n",
    "# defining the network\n",
    "net = EncapsulatedNTM(\n",
    "    num_inputs,\n",
    "    num_outputs,\n",
    "    controller_size,\n",
    "    controller_layers,\n",
    "    num_heads,\n",
    "    N,\n",
    "    M,\n",
    "    controller_= controller_, #FeedforwardController,\n",
    "    vanilla_heads=False\n",
    ")\n",
    "# net = EncapsulatedNTM\n",
    "device_ = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset(pd.read_csv(PATH_PRINTED_TRAIN_CSV), PATH_PRINTED_TRAIN_IMGS)\n",
    "val_dataset = dataset(pd.read_csv(PATH_PRINTED_VAL_CSV), PATH_PRINTED_VAL_IMGS)\n",
    "\n",
    "# creating dataloader\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\work\\self\\thesis\\win_ntm\\humanlike-ocr\\ntm\\ntm.py:105: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.softmax(o), self.state\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 0.098, train acc 0.022\n",
      "Loss train 0.098,  validation 0.111,  val acc 0.026\n",
      "Epoch 1\n",
      "epoch 1, loss 0.113, train acc 0.025\n",
      "Loss train 0.113,  validation 0.111,  val acc 0.025\n",
      "Epoch 2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 31\u001b[0m\n\u001b[0;32m     27\u001b[0m net\u001b[39m.\u001b[39minit_sequence(batch_size)\n\u001b[0;32m     30\u001b[0m training \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m outputs, _ \u001b[39m=\u001b[39m net(inputs\u001b[39m.\u001b[39;49mtype(torch\u001b[39m.\u001b[39;49mfloat))\n\u001b[0;32m     32\u001b[0m avg_loss \u001b[39m=\u001b[39m loss_fn(outputs, labels)\n\u001b[0;32m     34\u001b[0m list_outputs\u001b[39m.\u001b[39mappend(outputs)\n",
      "File \u001b[1;32md:\\work\\envs_win\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\work\\self\\thesis\\win_ntm\\humanlike-ocr\\ntm\\encapsulated.py:58\u001b[0m, in \u001b[0;36mEncapsulatedNTM.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     56\u001b[0m     x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_size, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_inputs, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat32)\n\u001b[0;32m     57\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mtype(torch\u001b[39m.\u001b[39mfloat)\n\u001b[1;32m---> 58\u001b[0m o, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprevious_state \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mntm(x, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprevious_state)\n\u001b[0;32m     59\u001b[0m \u001b[39mreturn\u001b[39;00m o, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprevious_state\n",
      "File \u001b[1;32md:\\work\\envs_win\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\work\\self\\thesis\\win_ntm\\humanlike-ocr\\ntm\\ntm.py:91\u001b[0m, in \u001b[0;36mNTM.forward\u001b[1;34m(self, x, prev_state, training)\u001b[0m\n\u001b[0;32m     89\u001b[0m         reads \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m [r]\n\u001b[0;32m     90\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 91\u001b[0m         head_state \u001b[39m=\u001b[39m head(controller_outp, prev_head_state)\n\u001b[0;32m     92\u001b[0m     heads_states \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m [head_state]\n\u001b[0;32m     94\u001b[0m \u001b[39m# Generate Output \u001b[39;00m\n",
      "File \u001b[1;32md:\\work\\envs_win\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\work\\self\\thesis\\win_ntm\\humanlike-ocr\\ntm\\head.py:231\u001b[0m, in \u001b[0;36mNTMWriteHead.forward\u001b[1;34m(self, embeddings, w_prev)\u001b[0m\n\u001b[0;32m    225\u001b[0m a \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39ma_reg(mat) \n\u001b[0;32m    227\u001b[0m \u001b[39m# e should be in [0, 1] \u001b[39;00m\n\u001b[0;32m    228\u001b[0m \u001b[39m# e = torch.sigmoid(e)\u001b[39;00m\n\u001b[0;32m    229\u001b[0m \n\u001b[0;32m    230\u001b[0m \u001b[39m# write to memory\u001b[39;00m\n\u001b[1;32m--> 231\u001b[0m w \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmemory\u001b[39m.\u001b[39;49maddress(k, beta, g, s, gamma, w_prev) \u001b[39m#self._address_memory(k, beta, g, s, gamma, w_prev)\u001b[39;00m\n\u001b[0;32m    232\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmemory\u001b[39m.\u001b[39mwrite(w, e, a)\n\u001b[0;32m    234\u001b[0m \u001b[39mreturn\u001b[39;00m w\n",
      "File \u001b[1;32md:\\work\\self\\thesis\\win_ntm\\humanlike-ocr\\ntm\\memory.py:87\u001b[0m, in \u001b[0;36mNTMMemory.address\u001b[1;34m(self, k, beta, g, s, gamma, w_prev)\u001b[0m\n\u001b[0;32m     85\u001b[0m wg \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpolate(w_prev, wc, g)\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice_)\n\u001b[0;32m     86\u001b[0m w_hat \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_shift(wg, s)\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice_)    \u001b[39m# convolutional shift\u001b[39;00m\n\u001b[1;32m---> 87\u001b[0m w \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sharpen(w_hat, gamma)\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice_)\n\u001b[0;32m     88\u001b[0m \u001b[39mreturn\u001b[39;00m w\n",
      "File \u001b[1;32md:\\work\\self\\thesis\\win_ntm\\humanlike-ocr\\ntm\\memory.py:106\u001b[0m, in \u001b[0;36mNTMMemory._sharpen\u001b[1;34m(self, w_hat, gamma)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_sharpen\u001b[39m(\u001b[39mself\u001b[39m, w_hat, gamma):\n\u001b[0;32m    105\u001b[0m     w \u001b[39m=\u001b[39m w_hat \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m gamma\n\u001b[1;32m--> 106\u001b[0m     w \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mdiv(w, torch\u001b[39m.\u001b[39;49msum(w, dim\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\u001b[39m.\u001b[39;49mview(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, \u001b[39m1\u001b[39;49m) \u001b[39m+\u001b[39;49m \u001b[39m1e-16\u001b[39;49m)\n\u001b[0;32m    107\u001b[0m     \u001b[39mreturn\u001b[39;00m w\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 50\n",
    "\n",
    "\n",
    "# training loop\n",
    "cnn_train_loss = []\n",
    "cnn_train_acc = []\n",
    "cnn_val_loss = []\n",
    "cnn_val_acc = []\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(\"Epoch {}\".format(epoch))\n",
    "\n",
    "    last_loss = 0.0\n",
    "    list_outputs = []\n",
    "    list_labels = []\n",
    "    \n",
    "    net.train()\n",
    "    for i, data in enumerate(train_dataloader):\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        inputs, labels = data[0].to(device_), data[1].to(device_)\n",
    "        # print(inputs.size())\n",
    "        labels = labels.type(torch.float)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        net.init_sequence(batch_size)\n",
    "\n",
    "\n",
    "        training = True\n",
    "        outputs, _ = net(inputs.type(torch.float))\n",
    "        avg_loss = loss_fn(outputs, labels)\n",
    "\n",
    "        list_outputs.append(outputs)\n",
    "        list_labels.append(torch.argmax(labels, dim=1))\n",
    "        \n",
    "        # avg_prec = metric(outputs, torch.argmax(labels, dim=1))\n",
    "        # print(torch.argmax(outputs, dim=1), torch.argmax(labels, dim=1))\n",
    "        avg_loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += avg_loss.item()\n",
    "        last_loss = running_loss\n",
    "        # last_avg_prec = avg_prec\n",
    "\n",
    "        # appending the train loss for grpah plotting\n",
    "        cnn_train_loss.append(avg_loss.item())\n",
    "\n",
    "    outputs = torch.cat(list_outputs, dim=0)\n",
    "    labels = torch.cat(list_labels).squeeze()\n",
    "    # print(labels)\n",
    "    # print(torch.argmax(outputs, dim=1))\n",
    "    acc = metric(outputs, labels)\n",
    "\n",
    "    # adding for the graph \n",
    "    cnn_train_acc.append(acc)\n",
    "\n",
    "    print(\"epoch {}, loss {:.3f}, train acc {:.3f}\".format(epoch, last_loss, acc))\n",
    "    # model_scripted = torch.jit.script(net)\n",
    "    # torch.save(net.state_dict(), \"model_state_convnet_01-06-23_.pt\")\n",
    "    # torch.save(net, \"model_convnet_{}_{}.pt\".format(\"12-05-23\", epoch))\n",
    "    \n",
    "    # validation\n",
    "    list_outputs = []\n",
    "    list_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        running_vloss = 0.0\n",
    "        for i, data in enumerate(val_dataloader):\n",
    "            net.init_sequence(batch_size)\n",
    "\n",
    "            vinputs, vlabels = data[0].to(device_), data[1].to(device_)\n",
    "            vlabels = vlabels.type(torch.float)\n",
    "            voutputs, _ = net(vinputs.type(torch.float))\n",
    "            list_outputs.append(voutputs)\n",
    "            list_labels.append(torch.argmax(vlabels, dim=1))\n",
    "            vloss = loss_fn(voutputs, vlabels)\n",
    "            running_vloss += vloss\n",
    "\n",
    "            # appending the val loss for the graph plotting\n",
    "            cnn_val_loss.append(vloss.item())\n",
    "\n",
    "        voutputs = torch.cat(list_outputs, dim=0)\n",
    "        vlabels = torch.cat(list_labels).squeeze()\n",
    "        vacc = metric(voutputs, vlabels)\n",
    "        \n",
    "        # appending the val accuracy for plotting graph\n",
    "        cnn_val_acc.append(vacc)\n",
    "\n",
    "        avg_vloss = running_vloss / (i+1)\n",
    "        print(\"Loss train {:.3f},  validation {:.3f},  val acc {:.3f}\".format(avg_loss, avg_vloss, vacc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "humanlike-ocr",
   "language": "python",
   "name": "humanlike-ocr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
