{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Saurabhbaghel/humanlike-ocr/blob/main/atomic_char_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8YVO6ox2TqNF",
        "outputId": "8d88bea6-4bf2-4e1d-ed03-508dcc0f88dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'humanlike-ocr'...\n",
            "remote: Enumerating objects: 364, done.\u001b[K\n",
            "remote: Counting objects: 100% (364/364), done.\u001b[K\n",
            "remote: Compressing objects: 100% (175/175), done.\u001b[K\n",
            "remote: Total 364 (delta 186), reused 364 (delta 186), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (364/364), 413.53 KiB | 4.86 MiB/s, done.\n",
            "Resolving deltas: 100% (186/186), done.\n"
          ]
        }
      ],
      "source": [
        "! git clone https://github.com/Saurabhbaghel/humanlike-ocr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LSME8DF9TzO6",
        "outputId": "5a139c52-a6f5-4b60-c65a-1ec7b123a320"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-04-09 18:38:05.218422: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-04-09 18:38:06.353548: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Using cache found in /root/.cache/torch/hub/facebookresearch_WSL-Images_main\n",
            "Using cache found in /root/.cache/torch/hub/facebookresearch_WSL-Images_main\n",
            "EPOCH 1\n",
            "/usr/local/lib/python3.9/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n",
            "batch 10 loss: 3.612033987045288\n",
            "batch 20 loss: 3.6103184461593627\n",
            "LOSS train 3.6103184461593627 valid 3.6151528358459473\n",
            "EPOCH 2\n",
            "batch 10 loss: 3.5812187671661375\n",
            "batch 20 loss: 3.565898966789246\n",
            "LOSS train 3.565898966789246 valid 3.7094149589538574\n",
            "EPOCH 3\n",
            "batch 10 loss: 3.5466625928878783\n",
            "batch 20 loss: 3.5246381759643555\n",
            "LOSS train 3.5246381759643555 valid 3.765072822570801\n",
            "EPOCH 4\n",
            "batch 10 loss: 3.531326985359192\n",
            "batch 20 loss: 3.509572696685791\n",
            "LOSS train 3.509572696685791 valid 3.802342414855957\n",
            "EPOCH 5\n",
            "batch 10 loss: 3.5242465496063233\n",
            "batch 20 loss: 3.5001095294952393\n",
            "LOSS train 3.5001095294952393 valid 3.823167324066162\n",
            "EPOCH 6\n",
            "batch 10 loss: 3.5185601472854615\n",
            "batch 20 loss: 3.4934890270233154\n",
            "LOSS train 3.4934890270233154 valid 3.8345255851745605\n",
            "EPOCH 7\n",
            "batch 10 loss: 3.51373085975647\n",
            "batch 20 loss: 3.4885518550872803\n",
            "LOSS train 3.4885518550872803 valid 3.8396155834198\n",
            "EPOCH 8\n",
            "batch 10 loss: 3.5096266746520994\n",
            "batch 20 loss: 3.4846616983413696\n",
            "LOSS train 3.4846616983413696 valid 3.8398022651672363\n",
            "EPOCH 9\n",
            "batch 10 loss: 3.506195640563965\n",
            "batch 20 loss: 3.4814427137374877\n",
            "LOSS train 3.4814427137374877 valid 3.8363938331604004\n",
            "EPOCH 10\n",
            "batch 10 loss: 3.5032691240310667\n",
            "batch 20 loss: 3.4785599946975707\n",
            "LOSS train 3.4785599946975707 valid 3.8258309364318848\n",
            "EPOCH 11\n",
            "batch 10 loss: 3.4980016469955446\n",
            "batch 20 loss: 3.4750058174133303\n",
            "LOSS train 3.4750058174133303 valid 3.7671725749969482\n",
            "EPOCH 12\n",
            "batch 10 loss: 3.4642974376678466\n",
            "batch 20 loss: 3.4679407596588137\n",
            "LOSS train 3.4679407596588137 valid 3.8304057121276855\n",
            "EPOCH 13\n",
            "batch 10 loss: 3.456630277633667\n",
            "batch 20 loss: 3.4642347574234007\n",
            "LOSS train 3.4642347574234007 valid 3.8247499465942383\n",
            "EPOCH 14\n",
            "batch 10 loss: 3.5208433151245115\n",
            "batch 20 loss: 3.4626882076263428\n",
            "LOSS train 3.4626882076263428 valid 3.813779830932617\n",
            "EPOCH 15\n",
            "batch 10 loss: 3.5102593421936037\n",
            "batch 20 loss: 3.462338852882385\n",
            "LOSS train 3.462338852882385 valid 3.8043055534362793\n",
            "EPOCH 16\n",
            "batch 10 loss: 3.50481059551239\n",
            "batch 20 loss: 3.4624849557876587\n",
            "LOSS train 3.4624849557876587 valid 3.796527862548828\n",
            "EPOCH 17\n",
            "batch 10 loss: 3.495457911491394\n",
            "batch 20 loss: 3.4667287826538087\n",
            "LOSS train 3.4667287826538087 valid 3.7902088165283203\n",
            "EPOCH 18\n",
            "batch 10 loss: 3.480695366859436\n",
            "batch 20 loss: 3.4458443880081178\n",
            "LOSS train 3.4458443880081178 valid 3.7919821739196777\n",
            "EPOCH 19\n",
            "batch 10 loss: 3.4765341520309447\n",
            "batch 20 loss: 3.4942739486694334\n",
            "LOSS train 3.4942739486694334 valid 3.8096537590026855\n",
            "EPOCH 20\n",
            "batch 10 loss: 3.428247356414795\n",
            "batch 20 loss: 3.452330756187439\n",
            "LOSS train 3.452330756187439 valid 3.7997958660125732\n",
            "EPOCH 21\n",
            "batch 10 loss: 3.37586669921875\n",
            "batch 20 loss: 3.52218177318573\n",
            "LOSS train 3.52218177318573 valid 3.6475894451141357\n",
            "EPOCH 22\n",
            "batch 10 loss: 3.5789576530456544\n",
            "batch 20 loss: 3.542468476295471\n",
            "LOSS train 3.542468476295471 valid 3.773235321044922\n",
            "EPOCH 23\n",
            "batch 10 loss: 3.5032002210617064\n",
            "batch 20 loss: 3.4928701877593995\n",
            "LOSS train 3.4928701877593995 valid 3.8192527294158936\n",
            "EPOCH 24\n",
            "batch 10 loss: 3.486810827255249\n",
            "batch 20 loss: 3.475484037399292\n",
            "LOSS train 3.475484037399292 valid 3.8119304180145264\n",
            "EPOCH 25\n",
            "batch 10 loss: 3.4871592044830324\n",
            "batch 20 loss: 3.472029423713684\n",
            "LOSS train 3.472029423713684 valid 3.798563003540039\n",
            "EPOCH 26\n",
            "batch 10 loss: 3.4883550643920898\n",
            "batch 20 loss: 3.468973684310913\n",
            "LOSS train 3.468973684310913 valid 3.78928279876709\n",
            "EPOCH 27\n",
            "batch 10 loss: 3.4890347003936766\n",
            "batch 20 loss: 3.466187620162964\n",
            "LOSS train 3.466187620162964 valid 3.783966541290283\n",
            "EPOCH 28\n",
            "batch 10 loss: 3.488158345222473\n",
            "batch 20 loss: 3.462229871749878\n",
            "LOSS train 3.462229871749878 valid 3.7806396484375\n",
            "EPOCH 29\n",
            "batch 10 loss: 3.480874252319336\n",
            "batch 20 loss: 3.452381420135498\n",
            "LOSS train 3.452381420135498 valid 3.7798049449920654\n",
            "EPOCH 30\n",
            "batch 10 loss: 3.452080011367798\n",
            "batch 20 loss: 3.4341449975967406\n",
            "LOSS train 3.4341449975967406 valid 3.7803797721862793\n",
            "EPOCH 31\n",
            "batch 10 loss: 3.380929112434387\n",
            "batch 20 loss: 3.432395577430725\n",
            "LOSS train 3.432395577430725 valid 3.8228793144226074\n",
            "EPOCH 32\n",
            "batch 10 loss: 3.3586883544921875\n",
            "batch 20 loss: 3.3503337860107423\n",
            "LOSS train 3.3503337860107423 valid 3.7193875312805176\n",
            "EPOCH 33\n",
            "batch 10 loss: 3.315137577056885\n",
            "batch 20 loss: 3.4229575395584106\n",
            "LOSS train 3.4229575395584106 valid 3.7256317138671875\n",
            "EPOCH 34\n",
            "batch 10 loss: 3.4625728130340576\n",
            "batch 20 loss: 3.4953147172927856\n",
            "LOSS train 3.4953147172927856 valid 3.8832783699035645\n",
            "EPOCH 35\n",
            "batch 10 loss: 3.515126371383667\n",
            "batch 20 loss: 3.4875170469284056\n",
            "LOSS train 3.4875170469284056 valid 3.8761978149414062\n",
            "EPOCH 36\n",
            "batch 10 loss: 3.508327770233154\n",
            "batch 20 loss: 3.480562210083008\n",
            "LOSS train 3.480562210083008 valid 3.8660707473754883\n",
            "EPOCH 37\n",
            "batch 10 loss: 3.4947159767150877\n",
            "batch 20 loss: 3.4731559276580812\n",
            "LOSS train 3.4731559276580812 valid 3.8388781547546387\n",
            "EPOCH 38\n",
            "batch 10 loss: 3.4904971361160277\n",
            "batch 20 loss: 3.468656611442566\n",
            "LOSS train 3.468656611442566 valid 3.801992416381836\n",
            "EPOCH 39\n",
            "batch 10 loss: 3.4924338817596436\n",
            "batch 20 loss: 3.465899634361267\n",
            "LOSS train 3.465899634361267 valid 3.78122878074646\n",
            "EPOCH 40\n",
            "batch 10 loss: 3.492264747619629\n",
            "batch 20 loss: 3.4617947816848753\n",
            "LOSS train 3.4617947816848753 valid 3.774261474609375\n",
            "EPOCH 41\n",
            "batch 10 loss: 3.4823541402816773\n",
            "batch 20 loss: 3.4426169633865356\n",
            "LOSS train 3.4426169633865356 valid 3.7741260528564453\n",
            "EPOCH 42\n",
            "batch 10 loss: 3.4651273488998413\n",
            "batch 20 loss: 3.4070013046264647\n",
            "LOSS train 3.4070013046264647 valid 3.745903968811035\n",
            "EPOCH 43\n",
            "batch 10 loss: 3.427284026145935\n",
            "batch 20 loss: 3.495960164070129\n",
            "LOSS train 3.495960164070129 valid 3.852050542831421\n",
            "EPOCH 44\n",
            "batch 10 loss: 3.517842507362366\n",
            "batch 20 loss: 3.4857373237609863\n",
            "LOSS train 3.4857373237609863 valid 3.836887836456299\n",
            "EPOCH 45\n",
            "batch 10 loss: 3.508406734466553\n",
            "batch 20 loss: 3.4794726371765137\n",
            "LOSS train 3.4794726371765137 valid 3.821584463119507\n",
            "EPOCH 46\n",
            "batch 10 loss: 3.49694447517395\n",
            "batch 20 loss: 3.473164916038513\n",
            "LOSS train 3.473164916038513 valid 3.8100109100341797\n",
            "EPOCH 47\n",
            "batch 10 loss: 3.4953386306762697\n",
            "batch 20 loss: 3.469289040565491\n",
            "LOSS train 3.469289040565491 valid 3.800288677215576\n",
            "EPOCH 48\n",
            "batch 10 loss: 3.4943350553512573\n",
            "batch 20 loss: 3.4664478302001953\n",
            "LOSS train 3.4664478302001953 valid 3.794457197189331\n",
            "EPOCH 49\n",
            "batch 10 loss: 3.4941582679748535\n",
            "batch 20 loss: 3.4643264770507813\n",
            "LOSS train 3.4643264770507813 valid 3.789978504180908\n",
            "EPOCH 50\n",
            "batch 10 loss: 3.4939324617385865\n",
            "batch 20 loss: 3.4623549938201905\n",
            "LOSS train 3.4623549938201905 valid 3.7848775386810303\n",
            "EPOCH 51\n",
            "batch 10 loss: 3.4928988933563234\n",
            "batch 20 loss: 3.460297107696533\n",
            "LOSS train 3.460297107696533 valid 3.7826943397521973\n",
            "EPOCH 52\n",
            "batch 10 loss: 3.490130400657654\n",
            "batch 20 loss: 3.45775625705719\n",
            "LOSS train 3.45775625705719 valid 3.7835450172424316\n",
            "EPOCH 53\n",
            "batch 10 loss: 3.480097007751465\n",
            "batch 20 loss: 3.4454954862594604\n",
            "LOSS train 3.4454954862594604 valid 3.7931723594665527\n",
            "EPOCH 54\n",
            "batch 10 loss: 3.43355712890625\n",
            "batch 20 loss: 3.397013020515442\n",
            "LOSS train 3.397013020515442 valid 3.8270697593688965\n",
            "EPOCH 55\n",
            "batch 10 loss: 3.369109845161438\n",
            "batch 20 loss: 3.4226999521255492\n",
            "LOSS train 3.4226999521255492 valid 3.8473739624023438\n",
            "EPOCH 56\n",
            "batch 10 loss: 3.528380012512207\n",
            "batch 20 loss: 3.475400924682617\n",
            "LOSS train 3.475400924682617 valid 3.83524227142334\n",
            "EPOCH 57\n",
            "batch 10 loss: 3.5221880197525026\n",
            "batch 20 loss: 3.472021746635437\n",
            "LOSS train 3.472021746635437 valid 3.819812774658203\n",
            "EPOCH 58\n",
            "batch 10 loss: 3.506140637397766\n",
            "batch 20 loss: 3.468903923034668\n",
            "LOSS train 3.468903923034668 valid 3.8034636974334717\n",
            "EPOCH 59\n",
            "batch 10 loss: 3.497068500518799\n",
            "batch 20 loss: 3.4661130905151367\n",
            "LOSS train 3.4661130905151367 valid 3.792309522628784\n",
            "EPOCH 60\n",
            "batch 10 loss: 3.496000146865845\n",
            "batch 20 loss: 3.464437174797058\n",
            "LOSS train 3.464437174797058 valid 3.7845191955566406\n",
            "EPOCH 61\n",
            "batch 10 loss: 3.495135045051575\n",
            "batch 20 loss: 3.462953495979309\n",
            "LOSS train 3.462953495979309 valid 3.779000759124756\n",
            "EPOCH 62\n",
            "batch 10 loss: 3.4946577072143556\n",
            "batch 20 loss: 3.4619934797286986\n",
            "LOSS train 3.4619934797286986 valid 3.77540922164917\n",
            "EPOCH 63\n",
            "batch 10 loss: 3.494506311416626\n",
            "batch 20 loss: 3.4613702297210693\n",
            "LOSS train 3.4613702297210693 valid 3.773008108139038\n",
            "EPOCH 64\n",
            "batch 10 loss: 3.4944658041000367\n",
            "batch 20 loss: 3.4609476566314696\n",
            "LOSS train 3.4609476566314696 valid 3.7712464332580566\n",
            "EPOCH 65\n",
            "batch 10 loss: 3.494452476501465\n",
            "batch 20 loss: 3.460653209686279\n",
            "LOSS train 3.460653209686279 valid 3.7698590755462646\n",
            "EPOCH 66\n",
            "batch 10 loss: 3.4944400787353516\n",
            "batch 20 loss: 3.460444521903992\n",
            "LOSS train 3.460444521903992 valid 3.7687175273895264\n",
            "EPOCH 67\n",
            "batch 10 loss: 3.494420886039734\n",
            "batch 20 loss: 3.4602957248687742\n",
            "LOSS train 3.4602957248687742 valid 3.7677552700042725\n",
            "EPOCH 68\n",
            "batch 10 loss: 3.494392991065979\n",
            "batch 20 loss: 3.460190176963806\n",
            "LOSS train 3.460190176963806 valid 3.7669339179992676\n",
            "EPOCH 69\n",
            "batch 10 loss: 3.494356894493103\n",
            "batch 20 loss: 3.4601169347763063\n",
            "LOSS train 3.4601169347763063 valid 3.7662291526794434\n",
            "EPOCH 70\n",
            "batch 10 loss: 3.4943137407302856\n",
            "batch 20 loss: 3.460068106651306\n",
            "LOSS train 3.460068106651306 valid 3.765625\n",
            "EPOCH 71\n",
            "batch 10 loss: 3.4942648887634276\n",
            "batch 20 loss: 3.460038113594055\n",
            "LOSS train 3.460038113594055 valid 3.765110969543457\n",
            "EPOCH 72\n",
            "batch 10 loss: 3.4942116737365723\n",
            "batch 20 loss: 3.4600225925445556\n",
            "LOSS train 3.4600225925445556 valid 3.7646758556365967\n",
            "EPOCH 73\n",
            "batch 10 loss: 3.494154930114746\n",
            "batch 20 loss: 3.4600185632705687\n",
            "LOSS train 3.4600185632705687 valid 3.764315605163574\n",
            "EPOCH 74\n",
            "batch 10 loss: 3.4940959215164185\n",
            "batch 20 loss: 3.460023212432861\n",
            "LOSS train 3.460023212432861 valid 3.7640256881713867\n",
            "EPOCH 75\n",
            "batch 10 loss: 3.4940354347229006\n",
            "batch 20 loss: 3.460034656524658\n",
            "LOSS train 3.460034656524658 valid 3.7638025283813477\n",
            "EPOCH 76\n",
            "batch 10 loss: 3.493973994255066\n",
            "batch 20 loss: 3.4600513219833373\n",
            "LOSS train 3.4600513219833373 valid 3.7636427879333496\n",
            "EPOCH 77\n",
            "batch 10 loss: 3.4939122200012207\n",
            "batch 20 loss: 3.4600719213485718\n",
            "LOSS train 3.4600719213485718 valid 3.763542890548706\n",
            "EPOCH 78\n",
            "batch 10 loss: 3.493850564956665\n",
            "batch 20 loss: 3.4600954055786133\n",
            "LOSS train 3.4600954055786133 valid 3.7635021209716797\n",
            "EPOCH 79\n",
            "batch 10 loss: 3.493789291381836\n",
            "batch 20 loss: 3.4601208925247193\n",
            "LOSS train 3.4601208925247193 valid 3.763517141342163\n",
            "EPOCH 80\n",
            "batch 10 loss: 3.4937288045883177\n",
            "batch 20 loss: 3.4601476430892943\n",
            "LOSS train 3.4601476430892943 valid 3.76358699798584\n",
            "EPOCH 81\n",
            "batch 10 loss: 3.4936691761016845\n",
            "batch 20 loss: 3.460175108909607\n",
            "LOSS train 3.460175108909607 valid 3.7637076377868652\n",
            "EPOCH 82\n",
            "batch 10 loss: 3.4936105728149416\n",
            "batch 20 loss: 3.460202622413635\n",
            "LOSS train 3.460202622413635 valid 3.763875722885132\n",
            "EPOCH 83\n",
            "batch 10 loss: 3.493553066253662\n",
            "batch 20 loss: 3.4602296352386475\n",
            "LOSS train 3.4602296352386475 valid 3.764087677001953\n",
            "EPOCH 84\n",
            "batch 10 loss: 3.4934970140457153\n",
            "batch 20 loss: 3.460255742073059\n",
            "LOSS train 3.460255742073059 valid 3.764333486557007\n",
            "EPOCH 85\n",
            "batch 10 loss: 3.4934423208236693\n",
            "batch 20 loss: 3.4602799892425535\n",
            "LOSS train 3.4602799892425535 valid 3.7646031379699707\n",
            "EPOCH 86\n",
            "batch 10 loss: 3.493389678001404\n",
            "batch 20 loss: 3.4603013515472414\n",
            "LOSS train 3.4603013515472414 valid 3.7648863792419434\n",
            "EPOCH 87\n",
            "batch 10 loss: 3.493340086936951\n",
            "batch 20 loss: 3.460317516326904\n",
            "LOSS train 3.460317516326904 valid 3.7651753425598145\n",
            "EPOCH 88\n",
            "batch 10 loss: 3.4932939052581786\n",
            "batch 20 loss: 3.460323429107666\n",
            "LOSS train 3.460323429107666 valid 3.765458106994629\n",
            "EPOCH 89\n",
            "batch 10 loss: 3.4932517051696776\n",
            "batch 20 loss: 3.460308837890625\n",
            "LOSS train 3.460308837890625 valid 3.7657155990600586\n",
            "EPOCH 90\n",
            "batch 10 loss: 3.4932132959365845\n",
            "batch 20 loss: 3.460247755050659\n",
            "LOSS train 3.460247755050659 valid 3.7659010887145996\n",
            "EPOCH 91\n",
            "batch 10 loss: 3.4931464195251465\n",
            "batch 20 loss: 3.4600566625595093\n",
            "LOSS train 3.4600566625595093 valid 3.76590895652771\n",
            "EPOCH 92\n",
            "batch 10 loss: 3.492768168449402\n",
            "batch 20 loss: 3.4593451976776124\n",
            "LOSS train 3.4593451976776124 valid 3.76607608795166\n",
            "EPOCH 93\n",
            "batch 10 loss: 3.49100775718689\n",
            "batch 20 loss: 3.45701117515564\n",
            "LOSS train 3.45701117515564 valid 3.7676312923431396\n",
            "EPOCH 94\n",
            "batch 10 loss: 3.4871434450149534\n",
            "batch 20 loss: 3.452758479118347\n",
            "LOSS train 3.452758479118347 valid 3.774679183959961\n",
            "EPOCH 95\n",
            "batch 10 loss: 3.476049852371216\n",
            "batch 20 loss: 3.4508117198944093\n",
            "LOSS train 3.4508117198944093 valid 3.777045965194702\n",
            "EPOCH 96\n",
            "batch 10 loss: 3.4624495029449465\n",
            "batch 20 loss: 3.4532204151153563\n",
            "LOSS train 3.4532204151153563 valid 3.8426949977874756\n",
            "EPOCH 97\n",
            "batch 10 loss: 3.4921407461166383\n",
            "batch 20 loss: 3.4860023975372316\n",
            "LOSS train 3.4860023975372316 valid 3.811319351196289\n",
            "EPOCH 98\n",
            "batch 10 loss: 3.481610083580017\n",
            "batch 20 loss: 3.478657054901123\n",
            "LOSS train 3.478657054901123 valid 3.7787156105041504\n",
            "EPOCH 99\n",
            "batch 10 loss: 3.4844882011413576\n",
            "batch 20 loss: 3.473025608062744\n",
            "LOSS train 3.473025608062744 valid 3.774298667907715\n",
            "EPOCH 100\n",
            "batch 10 loss: 3.486765003204346\n",
            "batch 20 loss: 3.46930878162384\n",
            "LOSS train 3.46930878162384 valid 3.7727181911468506\n"
          ]
        }
      ],
      "source": [
        "! python /content/humanlike-ocr/training_atomic_char.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3qi9lz_pUwG3"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPpCXyYe+yXFaUO8yoG1BFz",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}