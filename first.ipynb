{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision as tv\n",
    "import cv2 as cv\n",
    "from ntm.ntm import NTM\n",
    "from ntm.controller import LSTMController\n",
    "from ntm.head import NTMWriteHead, NTMReadHead\n",
    "from ntm.memory import NTMMemory\n",
    "from ntm.encapsulated import EncapsulatedNTM\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, M = 120, 120\n",
    "\n",
    "num_inputs = 80\n",
    "num_outputs = 32\n",
    "num_layers = 2\n",
    "\n",
    "controller_size = 100\n",
    "num_heads = 1\n",
    "\n",
    "# num_inputs + M * num_heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7ff3e5076da0>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "v = tv.io.image.decode_jpeg(enc_char1)\n",
    "plt.imshow(v.permute(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "Im= tv.utils.Image.Image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell = EncapsulatedNTM(\n",
    "    num_inputs=num_inputs,\n",
    "    num_outputs=num_outputs,\n",
    "    controller_size=controller_size,\n",
    "    controller_layers=num_layers,\n",
    "    num_heads=1,\n",
    "    N=N,\n",
    "    M=M\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell.init_sequence(batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 120, 120])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cell.memory.memory.size()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>529</th>\n",
       "      <th>429</th>\n",
       "      <th>600</th>\n",
       "      <th>1583</th>\n",
       "      <th>1003</th>\n",
       "      <th>1708</th>\n",
       "      <th>38</th>\n",
       "      <th>793</th>\n",
       "      <th>1020</th>\n",
       "      <th>1503</th>\n",
       "      <th>...</th>\n",
       "      <th>575</th>\n",
       "      <th>1542</th>\n",
       "      <th>1126</th>\n",
       "      <th>899</th>\n",
       "      <th>1522</th>\n",
       "      <th>741</th>\n",
       "      <th>275</th>\n",
       "      <th>1123</th>\n",
       "      <th>1757</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.160751</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.164665</td>\n",
       "      <td>0.411528</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.347005</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.237509</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.360355</td>\n",
       "      <td>0.036318</td>\n",
       "      <td>0.016661</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.535782</td>\n",
       "      <td>2.408298</td>\n",
       "      <td>1.104285</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.109834</td>\n",
       "      <td>1.341766</td>\n",
       "      <td>...</td>\n",
       "      <td>0.066502</td>\n",
       "      <td>1.195841</td>\n",
       "      <td>1.017552</td>\n",
       "      <td>0.306543</td>\n",
       "      <td>4.660774</td>\n",
       "      <td>1.194988</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.320019</td>\n",
       "      <td>0.711735</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.235436</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.126883</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.837503</td>\n",
       "      <td>0.077218</td>\n",
       "      <td>0.948346</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.235417</td>\n",
       "      <td>0.084102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.219687</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.676321</td>\n",
       "      <td>0.088754</td>\n",
       "      <td>1.581639</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.398123</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.911793</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.052177</td>\n",
       "      <td>0.786023</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.076559</td>\n",
       "      <td>0.198894</td>\n",
       "      <td>0.640127</td>\n",
       "      <td>0.829963</td>\n",
       "      <td>0.335316</td>\n",
       "      <td>1.086302</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.460200</td>\n",
       "      <td>1.152315</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.431127</td>\n",
       "      <td>0.461240</td>\n",
       "      <td>0.716281</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.798825</td>\n",
       "      <td>1.207885</td>\n",
       "      <td>...</td>\n",
       "      <td>1.277711</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.306082</td>\n",
       "      <td>0.096395</td>\n",
       "      <td>0.039457</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.185793</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>0.166207</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.015810</td>\n",
       "      <td>0.709898</td>\n",
       "      <td>0.070699</td>\n",
       "      <td>1.133485</td>\n",
       "      <td>0.606619</td>\n",
       "      <td>0.229006</td>\n",
       "      <td>1.744559</td>\n",
       "      <td>0.409903</td>\n",
       "      <td>...</td>\n",
       "      <td>0.143646</td>\n",
       "      <td>1.075151</td>\n",
       "      <td>3.338962</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.684950</td>\n",
       "      <td>0.419209</td>\n",
       "      <td>1.449569</td>\n",
       "      <td>0.153156</td>\n",
       "      <td>0.400220</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>0.756977</td>\n",
       "      <td>0.456375</td>\n",
       "      <td>0.293851</td>\n",
       "      <td>1.986922</td>\n",
       "      <td>1.946342</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.198460</td>\n",
       "      <td>0.863284</td>\n",
       "      <td>0.731359</td>\n",
       "      <td>0.981429</td>\n",
       "      <td>...</td>\n",
       "      <td>0.456500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.755649</td>\n",
       "      <td>0.523964</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.460823</td>\n",
       "      <td>2.361780</td>\n",
       "      <td>0.012915</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>0.291869</td>\n",
       "      <td>0.263729</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.227685</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008934</td>\n",
       "      <td>1.983933</td>\n",
       "      <td>0.247986</td>\n",
       "      <td>...</td>\n",
       "      <td>0.693777</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.558063</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.861166</td>\n",
       "      <td>0.574466</td>\n",
       "      <td>0.095937</td>\n",
       "      <td>0.538186</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.360662</td>\n",
       "      <td>0.322281</td>\n",
       "      <td>...</td>\n",
       "      <td>0.570556</td>\n",
       "      <td>0.996321</td>\n",
       "      <td>0.174717</td>\n",
       "      <td>0.800462</td>\n",
       "      <td>0.665416</td>\n",
       "      <td>1.788651</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.185145</td>\n",
       "      <td>1.043756</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>0.052192</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.133737</td>\n",
       "      <td>0.202843</td>\n",
       "      <td>0.113304</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.452553</td>\n",
       "      <td>0.684665</td>\n",
       "      <td>0.905403</td>\n",
       "      <td>...</td>\n",
       "      <td>1.517403</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>160 rows Ã— 81 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          529       429       600      1583      1003      1708        38  \\\n",
       "0    0.160751  0.000000  0.000000  1.164665  0.411528  0.000000  0.000000   \n",
       "1    0.360355  0.036318  0.016661  0.000000  2.535782  2.408298  1.104285   \n",
       "2    0.000000  0.235436  0.000000  0.126883  0.000000  0.000000  0.000000   \n",
       "3    0.000000  0.676321  0.088754  1.581639  0.000000  0.398123  0.000000   \n",
       "4    0.000000  0.460200  1.152315  0.000000  1.431127  0.461240  0.716281   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "155  0.166207  0.000000  1.015810  0.709898  0.070699  1.133485  0.606619   \n",
       "156  0.756977  0.456375  0.293851  1.986922  1.946342  0.000000  1.198460   \n",
       "157  0.291869  0.263729  0.000000  0.000000  0.227685  0.000000  0.000000   \n",
       "158  0.000000  0.861166  0.574466  0.095937  0.538186  0.000000  0.000000   \n",
       "159  0.052192  0.000000  0.133737  0.202843  0.113304  0.000000  0.000000   \n",
       "\n",
       "          793      1020      1503  ...       575      1542      1126  \\\n",
       "0    0.000000  2.347005  0.000000  ...  0.237509  0.000000  0.000000   \n",
       "1    0.000000  0.109834  1.341766  ...  0.066502  1.195841  1.017552   \n",
       "2    0.837503  0.077218  0.948346  ...  0.000000  0.235417  0.084102   \n",
       "3    0.000000  0.911793  0.000000  ...  0.052177  0.786023  0.000000   \n",
       "4    0.000000  0.798825  1.207885  ...  1.277711  0.000000  3.306082   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "155  0.229006  1.744559  0.409903  ...  0.143646  1.075151  3.338962   \n",
       "156  0.863284  0.731359  0.981429  ...  0.456500  0.000000  0.000000   \n",
       "157  0.008934  1.983933  0.247986  ...  0.693777  0.000000  0.000000   \n",
       "158  0.000000  1.360662  0.322281  ...  0.570556  0.996321  0.174717   \n",
       "159  0.452553  0.684665  0.905403  ...  1.517403  0.000000  0.000000   \n",
       "\n",
       "          899      1522       741       275      1123      1757  labels  \n",
       "0    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000       0  \n",
       "1    0.306543  4.660774  1.194988  0.000000  1.320019  0.711735       1  \n",
       "2    0.000000  0.000000  0.000000  0.000000  0.000000  1.219687       2  \n",
       "3    1.076559  0.198894  0.640127  0.829963  0.335316  1.086302       4  \n",
       "4    0.096395  0.039457  0.000000  0.000000  0.000000  1.185793       5  \n",
       "..        ...       ...       ...       ...       ...       ...     ...  \n",
       "155  0.000000  0.684950  0.419209  1.449569  0.153156  0.400220      32  \n",
       "156  0.755649  0.523964  0.000000  0.460823  2.361780  0.012915      33  \n",
       "157  0.000000  0.000000  0.000000  0.558063  0.000000  0.000000      34  \n",
       "158  0.800462  0.665416  1.788651  0.000000  1.185145  1.043756      35  \n",
       "159  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000      36  \n",
       "\n",
       "[160 rows x 81 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_path = \"data/final_80_features.csv\"\n",
    "features_and_labels = pd.read_csv(csv_path)\n",
    "\n",
    "features_and_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_wo_labels=features_and_labels.columns[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # to get the features of the images with unique labels\n",
    "# labels_ = features_and_labels[\"labels\"].unique()\n",
    "\n",
    "# # \n",
    "# for label in labels_:\n",
    "#     features_and_labels.loc[features_and_labels[\"labels\"] == label][0,:] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_ = features_and_labels.head(32)\n",
    "features_ndarray = f_[cols_wo_labels].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 80)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_ndarray.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_char = features_ndarray[:4]\n",
    "first_char= torch.from_numpy(first_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 80])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_char.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/ashatya/Data/work/envs/torch/lib/python3.10/site-packages/torch/nn/functional.py:1967: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[0.5103, 0.4762, 0.4961, 0.4858, 0.5386, 0.5137, 0.5113, 0.5053, 0.4942,\n",
       "          0.4996, 0.4781, 0.5088, 0.4852, 0.4970, 0.5089, 0.5147, 0.4609, 0.4784,\n",
       "          0.5058, 0.5103, 0.5138, 0.5046, 0.4997, 0.5132, 0.4856, 0.5079, 0.4725,\n",
       "          0.5046, 0.4994, 0.4945, 0.4892, 0.4884],\n",
       "         [0.4807, 0.4603, 0.5068, 0.4814, 0.5550, 0.5109, 0.5225, 0.4817, 0.4890,\n",
       "          0.4745, 0.4717, 0.4552, 0.5010, 0.4877, 0.5165, 0.4807, 0.4807, 0.4689,\n",
       "          0.5038, 0.5135, 0.5053, 0.5285, 0.4745, 0.4707, 0.4833, 0.5020, 0.4552,\n",
       "          0.5080, 0.4940, 0.5282, 0.4587, 0.4875],\n",
       "         [0.4746, 0.4668, 0.5086, 0.5078, 0.5343, 0.4991, 0.5281, 0.4779, 0.4973,\n",
       "          0.4883, 0.4786, 0.5084, 0.4842, 0.4900, 0.5275, 0.5193, 0.4903, 0.5170,\n",
       "          0.4928, 0.5149, 0.4996, 0.4907, 0.5023, 0.5077, 0.4821, 0.5142, 0.4800,\n",
       "          0.5083, 0.5116, 0.4847, 0.4944, 0.5098],\n",
       "         [0.4869, 0.4689, 0.5124, 0.4638, 0.5507, 0.5303, 0.5209, 0.4919, 0.5207,\n",
       "          0.4925, 0.4487, 0.4867, 0.5105, 0.4928, 0.5235, 0.5056, 0.4763, 0.4931,\n",
       "          0.4938, 0.5127, 0.4884, 0.5291, 0.4830, 0.5158, 0.4735, 0.5005, 0.4617,\n",
       "          0.4832, 0.5290, 0.4778, 0.4935, 0.4969]], grad_fn=<SigmoidBackward0>),\n",
       " ([tensor([[-2.7491e-03,  2.1907e-03, -5.7637e-04,  1.8236e-03,  5.6064e-04,\n",
       "             8.3981e-03, -2.0889e-03,  3.4874e-03,  8.4336e-04,  1.8477e-03,\n",
       "            -3.3845e-03, -3.0123e-03, -5.0553e-04,  4.0690e-05,  5.9013e-03,\n",
       "            -5.7582e-03, -6.8799e-03,  6.8589e-03,  4.7217e-03, -6.4465e-03,\n",
       "            -5.8116e-03,  3.3367e-03,  4.0321e-03, -1.4436e-03,  1.8429e-03,\n",
       "            -8.4886e-03, -1.5783e-03, -3.2974e-03,  6.8629e-04,  1.2188e-03,\n",
       "            -1.2031e-03, -7.0318e-03,  8.2870e-04, -3.9354e-03,  2.0004e-03,\n",
       "             5.9339e-03,  6.1517e-04, -1.6353e-03, -4.4527e-03, -3.2346e-03,\n",
       "             4.2631e-03, -1.7768e-03,  2.6416e-03, -1.2621e-03,  1.2822e-03,\n",
       "            -6.5146e-03, -3.7945e-03,  6.7439e-04, -3.2840e-03, -7.3091e-04,\n",
       "             5.2863e-04,  1.0205e-03,  2.6106e-03, -2.3044e-04,  9.9987e-04,\n",
       "             2.7856e-03, -5.7131e-03, -3.9436e-04, -5.3571e-03,  7.7919e-04,\n",
       "             4.8267e-03, -1.6468e-04,  1.1501e-03, -2.8745e-03, -2.0276e-03,\n",
       "             5.0633e-03, -2.8147e-03, -8.1567e-04,  2.1020e-03,  9.8432e-04,\n",
       "            -8.3781e-04, -7.9364e-04,  3.0809e-03, -1.0531e-03, -3.5430e-03,\n",
       "             2.7114e-03, -3.5832e-04,  3.3082e-03, -3.3625e-04, -2.5902e-03,\n",
       "            -4.8354e-03,  3.0273e-04,  1.0924e-04, -2.1188e-03, -4.4839e-04,\n",
       "            -1.1456e-03, -2.4128e-04,  2.8713e-03, -8.0628e-04,  6.4407e-03,\n",
       "             2.3328e-03, -6.4926e-03,  6.5556e-03, -2.7002e-03,  8.6880e-04,\n",
       "             8.3984e-04,  2.3901e-04, -3.7116e-03, -2.1834e-03, -8.5856e-04,\n",
       "            -5.9653e-04, -5.8911e-04, -6.5046e-03,  5.9349e-03, -2.3121e-03,\n",
       "             2.6013e-03,  1.2605e-03, -2.2292e-03, -2.3568e-03, -3.1588e-03,\n",
       "             1.0089e-03,  2.5469e-03, -4.0139e-03,  2.9207e-03, -2.2209e-04,\n",
       "            -3.8178e-03,  1.4284e-03,  2.2701e-03, -3.7189e-03, -8.5054e-04],\n",
       "           [-2.8658e-03,  2.1275e-03, -7.2136e-04,  1.8138e-03,  7.2680e-04,\n",
       "             8.6567e-03, -2.4748e-03,  3.7327e-03,  7.7842e-04,  1.1799e-03,\n",
       "            -3.1794e-03, -3.1455e-03, -9.0969e-04,  3.8485e-04,  5.5439e-03,\n",
       "            -5.7737e-03, -6.7729e-03,  6.6667e-03,  5.0158e-03, -6.2294e-03,\n",
       "            -5.8546e-03,  3.2685e-03,  4.1898e-03, -1.4900e-03,  2.1501e-03,\n",
       "            -8.2611e-03, -1.5388e-03, -3.7141e-03,  7.2255e-04,  1.3558e-03,\n",
       "            -9.6721e-04, -6.9184e-03,  6.7136e-04, -3.9970e-03,  1.5306e-03,\n",
       "             6.0752e-03,  8.1774e-04, -1.9494e-03, -4.5407e-03, -3.0002e-03,\n",
       "             3.9980e-03, -1.0985e-03,  2.0782e-03, -1.0592e-03,  1.8009e-03,\n",
       "            -6.7214e-03, -3.9192e-03,  6.0094e-04, -3.4628e-03, -9.7985e-04,\n",
       "             3.2379e-04,  1.1330e-03,  2.3956e-03, -1.3845e-04,  5.0787e-04,\n",
       "             2.6563e-03, -5.4773e-03, -7.0296e-04, -5.6767e-03,  1.3015e-04,\n",
       "             4.7804e-03,  2.3396e-05,  8.8802e-04, -2.3385e-03, -2.1696e-03,\n",
       "             5.2303e-03, -2.0931e-03, -1.4167e-03,  2.6347e-03,  1.0560e-03,\n",
       "            -9.9699e-04, -6.6573e-04,  2.8331e-03, -9.3369e-04, -3.7941e-03,\n",
       "             3.2351e-03,  3.2783e-04,  2.9621e-03,  2.0116e-05, -2.9903e-03,\n",
       "            -4.3150e-03,  4.1565e-04, -5.1465e-06, -1.8196e-03, -7.8601e-04,\n",
       "            -1.2218e-03, -3.1810e-04,  2.8189e-03, -7.6219e-04,  5.9344e-03,\n",
       "             2.6498e-03, -6.5090e-03,  6.4894e-03, -2.9084e-03,  1.2332e-03,\n",
       "             8.1702e-04, -4.3811e-04, -3.8219e-03, -2.2328e-03, -2.8398e-04,\n",
       "            -8.0933e-04, -6.2053e-04, -6.9861e-03,  5.7811e-03, -2.1951e-03,\n",
       "             2.5968e-03,  1.2859e-03, -2.2125e-03, -1.8131e-03, -3.1343e-03,\n",
       "             1.4077e-03,  2.6957e-03, -3.8838e-03,  2.4015e-03, -5.2974e-04,\n",
       "            -4.0927e-03,  1.4340e-03,  2.3422e-03, -3.8246e-03, -3.9153e-04],\n",
       "           [-2.8905e-03,  2.0308e-03, -7.5633e-04,  1.9099e-03,  5.3761e-04,\n",
       "             8.3854e-03, -2.3206e-03,  4.2841e-03,  9.4830e-04,  1.1208e-03,\n",
       "            -2.8593e-03, -3.1146e-03, -1.0135e-03,  5.3882e-04,  6.0044e-03,\n",
       "            -5.7151e-03, -7.2112e-03,  5.9560e-03,  4.6601e-03, -6.1624e-03,\n",
       "            -5.6736e-03,  3.4252e-03,  4.5054e-03, -1.6048e-03,  2.0752e-03,\n",
       "            -7.8394e-03, -1.5675e-03, -3.7475e-03,  9.5122e-04,  9.9853e-04,\n",
       "            -1.3540e-03, -6.7796e-03,  7.7874e-04, -3.6794e-03,  1.7606e-03,\n",
       "             5.9491e-03,  1.1284e-03, -1.6571e-03, -4.4234e-03, -3.4171e-03,\n",
       "             4.2968e-03, -1.7341e-03,  2.7251e-03, -8.1926e-04,  1.7653e-03,\n",
       "            -6.9940e-03, -3.6309e-03,  1.5114e-04, -3.1137e-03, -1.2563e-03,\n",
       "             1.9810e-04,  5.1069e-04,  2.4552e-03, -2.5122e-04,  1.4160e-03,\n",
       "             2.6335e-03, -5.5160e-03, -6.7487e-05, -5.5484e-03,  1.2236e-04,\n",
       "             4.7776e-03,  4.5805e-04,  1.2122e-03, -3.1636e-03, -3.0620e-03,\n",
       "             5.0041e-03, -1.8675e-03, -1.4753e-03,  2.6111e-03,  1.1515e-03,\n",
       "            -9.1773e-04, -6.3148e-04,  2.8985e-03, -1.3755e-03, -3.5844e-03,\n",
       "             2.8314e-03, -1.3886e-04,  3.0874e-03, -6.8831e-04, -2.7922e-03,\n",
       "            -4.4449e-03,  3.0397e-05,  4.8402e-04, -2.3307e-03, -1.0451e-03,\n",
       "            -1.3210e-03, -4.0457e-04,  2.8750e-03, -8.4823e-04,  6.8501e-03,\n",
       "             2.8635e-03, -6.3542e-03,  6.5200e-03, -1.7233e-03,  1.1265e-03,\n",
       "             1.0996e-03, -3.9644e-04, -4.0639e-03, -2.3633e-03, -6.4247e-04,\n",
       "            -1.0811e-03, -7.0775e-04, -6.7580e-03,  5.7738e-03, -2.1271e-03,\n",
       "             3.1983e-03,  1.5308e-03, -2.4093e-03, -2.0625e-03, -3.5621e-03,\n",
       "             1.0330e-03,  2.7879e-03, -3.2216e-03,  3.0399e-03, -2.8392e-04,\n",
       "            -4.1245e-03,  1.0154e-03,  2.2606e-03, -3.9098e-03, -8.6630e-04],\n",
       "           [-2.8575e-03,  1.8593e-03, -2.4780e-04,  1.9653e-03,  1.2548e-03,\n",
       "             8.5048e-03, -2.1783e-03,  3.5592e-03,  1.0091e-03,  1.5288e-03,\n",
       "            -3.7493e-03, -3.0702e-03, -6.9028e-05, -2.8730e-04,  5.6508e-03,\n",
       "            -5.4298e-03, -6.8465e-03,  6.7859e-03,  4.5153e-03, -6.3255e-03,\n",
       "            -5.7092e-03,  3.9106e-03,  4.0207e-03, -1.5460e-03,  1.7903e-03,\n",
       "            -8.5004e-03, -1.3743e-03, -3.1312e-03,  6.2929e-04,  1.2293e-03,\n",
       "            -1.1636e-03, -6.6799e-03,  4.9758e-04, -4.3400e-03,  2.0515e-03,\n",
       "             6.2714e-03,  5.3923e-04, -1.8512e-03, -4.7345e-03, -3.1028e-03,\n",
       "             4.1028e-03, -1.3882e-03,  2.6455e-03, -1.2595e-03,  1.4048e-03,\n",
       "            -6.6625e-03, -4.0662e-03,  8.9535e-04, -3.6198e-03, -5.2666e-04,\n",
       "             6.6993e-04,  6.2755e-04,  2.3736e-03, -2.6732e-04,  6.9649e-04,\n",
       "             3.0534e-03, -5.5671e-03, -4.8470e-04, -5.0624e-03,  8.2240e-04,\n",
       "             4.2417e-03, -5.2983e-04,  1.2277e-03, -2.7040e-03, -2.0532e-03,\n",
       "             5.3461e-03, -3.1537e-03, -9.7955e-04,  2.1373e-03,  8.1447e-04,\n",
       "            -2.5300e-04, -8.0211e-04,  2.9919e-03, -9.7063e-04, -3.1829e-03,\n",
       "             2.9083e-03, -4.0684e-04,  3.2403e-03, -3.9642e-04, -2.2836e-03,\n",
       "            -4.6851e-03,  5.5005e-04,  3.2249e-04, -2.2108e-03, -5.6123e-04,\n",
       "            -1.1539e-03,  2.1009e-04,  2.7631e-03, -7.1734e-04,  6.4044e-03,\n",
       "             2.7645e-03, -6.8023e-03,  6.4511e-03, -3.1657e-03,  7.7330e-04,\n",
       "             1.3177e-03,  6.3591e-05, -4.0523e-03, -2.0649e-03, -3.3450e-04,\n",
       "             1.0159e-05, -8.3979e-04, -6.2922e-03,  5.5764e-03, -2.4239e-03,\n",
       "             2.2453e-03,  1.3710e-03, -2.3371e-03, -2.6134e-03, -3.1958e-03,\n",
       "             1.3552e-03,  2.8859e-03, -3.8022e-03,  2.6942e-03, -7.0089e-05,\n",
       "            -4.4009e-03,  1.1904e-03,  2.0889e-03, -3.9075e-03, -8.3642e-04]],\n",
       "          grad_fn=<SqueezeBackward1>)],\n",
       "  (tensor([[[ 3.3569e-02,  1.9435e-01,  1.9906e-01,  1.2507e-01, -1.0348e-01,\n",
       "              1.8955e-02, -1.4227e-02, -1.6099e-01, -5.7120e-02, -2.4374e-02,\n",
       "              9.9499e-02,  5.5321e-04, -1.3257e-02,  9.6964e-02, -1.2416e-01,\n",
       "             -1.4523e-01,  1.1994e-01, -4.1234e-03, -1.2163e-01,  5.0143e-02,\n",
       "             -1.1174e-01,  1.2221e-01, -2.7299e-03, -1.3723e-01,  8.6901e-02,\n",
       "             -1.2675e-01,  3.2713e-02,  9.9578e-02, -1.4374e-01, -9.7309e-03,\n",
       "             -6.8845e-02,  1.0860e-02, -1.2342e-02,  1.8767e-01,  1.3552e-01,\n",
       "             -9.5652e-02,  3.8787e-03, -9.4204e-02,  3.2951e-01,  4.3427e-02,\n",
       "             -2.0715e-02, -1.3918e-01,  2.1911e-01, -9.1480e-02, -1.1319e-01,\n",
       "              2.9684e-01,  4.8269e-02, -4.3120e-03, -9.5692e-02, -1.6646e-01,\n",
       "              3.2416e-02, -3.4011e-02,  1.0096e-01, -2.8958e-02, -1.7847e-01,\n",
       "              4.4422e-02,  1.8173e-01,  6.6520e-02,  7.6806e-02,  5.0599e-02,\n",
       "              9.8621e-02, -1.4064e-01,  1.4613e-01,  1.0498e-02, -1.2367e-01,\n",
       "              2.2775e-01,  1.8476e-01,  1.5855e-01, -1.2091e-01,  1.9780e-02,\n",
       "              1.7667e-01, -7.5137e-02,  5.0625e-02, -3.3709e-02,  1.3833e-01,\n",
       "             -1.0128e-01,  9.6827e-02,  1.7356e-01, -1.9568e-02, -1.9284e-02,\n",
       "             -1.2311e-01,  1.6980e-01, -1.0084e-02,  8.2718e-02, -9.3180e-02,\n",
       "              8.6101e-02, -7.0884e-02, -6.2595e-02,  1.2807e-01, -1.4765e-01,\n",
       "             -2.4929e-02,  3.0795e-03, -1.3092e-01,  4.6930e-02,  9.5535e-02,\n",
       "              3.7861e-02, -4.6913e-02, -1.1213e-01, -7.8664e-02,  7.2292e-02],\n",
       "            [-4.9178e-01, -5.3159e-01,  5.1162e-02, -1.3741e-01,  4.3011e-02,\n",
       "              5.3091e-02,  2.2574e-01,  3.1482e-02,  4.3191e-02, -1.1605e-01,\n",
       "             -6.0801e-01, -5.5039e-02, -1.0231e-01,  6.8922e-03, -2.8135e-01,\n",
       "             -6.6683e-05,  1.8426e-01, -1.6369e-01,  3.4487e-02, -1.4039e-02,\n",
       "              5.5987e-01, -1.1732e-02,  1.2092e-02, -3.0555e-03, -4.2090e-03,\n",
       "              6.5015e-01, -2.1864e-03, -1.2517e-02, -4.8042e-02, -7.4796e-03,\n",
       "             -3.0483e-02,  4.5309e-01,  1.5419e-01, -1.0931e-01, -6.5250e-02,\n",
       "             -3.8067e-02, -8.2983e-02, -9.5834e-04, -6.6049e-01, -1.1980e-01,\n",
       "              1.3389e-02, -2.7023e-01,  2.4078e-01, -6.8289e-02,  1.6438e-03,\n",
       "              1.3282e-01,  4.8982e-01, -4.5046e-01, -2.1943e-01, -2.2590e-02,\n",
       "             -3.0995e-02,  1.1658e-05, -3.4848e-01,  1.7109e-01,  1.9700e-02,\n",
       "              4.7305e-02,  3.9993e-01, -3.0467e-01,  1.0208e-01,  3.6102e-01,\n",
       "              2.6556e-02, -2.0932e-02,  1.7888e-01,  1.6843e-02, -1.8449e-02,\n",
       "             -2.3059e-01, -7.4581e-02,  5.5942e-01,  2.7880e-02,  2.2623e-01,\n",
       "              3.3805e-01, -3.6494e-01, -9.8669e-03,  9.4995e-03,  6.6580e-01,\n",
       "              9.1465e-02, -3.6565e-01,  4.8498e-02,  2.4539e-03, -2.4454e-01,\n",
       "              8.7134e-03, -3.0427e-01,  8.5347e-02, -2.4789e-01,  3.8331e-02,\n",
       "             -1.2821e-03,  2.5322e-01,  2.3632e-02,  9.0083e-02,  1.3863e-02,\n",
       "             -2.5617e-01,  6.2489e-02, -3.1259e-02, -5.1653e-01, -3.4790e-02,\n",
       "              3.2534e-01,  6.8169e-01, -1.0319e-02, -2.7841e-02,  1.2623e-01],\n",
       "            [-1.7670e-02,  4.8048e-02,  2.8765e-01, -1.6899e-01, -8.2756e-02,\n",
       "             -1.7649e-02,  1.9401e-02,  8.9312e-02, -1.1913e-01, -2.4746e-01,\n",
       "              6.4632e-02, -1.8245e-01,  9.3570e-02,  2.3277e-01, -1.5719e-01,\n",
       "             -3.9931e-02,  4.0962e-02, -2.0381e-02,  9.5284e-03,  1.1269e-01,\n",
       "             -4.3583e-01,  4.4184e-01,  9.3247e-03, -2.2001e-01, -5.2210e-02,\n",
       "             -1.8218e-01,  3.5641e-01, -1.2963e-01, -8.2286e-02, -6.7971e-02,\n",
       "             -1.5275e-01,  1.8944e-02,  2.2015e-01,  1.5470e-01,  3.1688e-02,\n",
       "             -1.9637e-01, -1.1462e-01,  5.1751e-02, -1.5764e-01, -5.4158e-02,\n",
       "              5.4486e-02, -4.2858e-02, -1.8699e-01,  4.1869e-02,  3.0258e-01,\n",
       "              2.0483e-01,  8.4443e-02, -1.4763e-01, -6.1509e-02,  8.1636e-02,\n",
       "             -8.8668e-02, -1.4574e-01,  1.6962e-01,  1.8242e-02, -2.6515e-01,\n",
       "             -1.8323e-01,  8.0255e-02,  9.4385e-02,  3.0442e-02,  1.8770e-02,\n",
       "              3.8498e-01, -1.4758e-01,  3.1113e-02, -9.1608e-04, -2.2492e-01,\n",
       "             -5.1773e-02,  1.4129e-01,  1.6778e-02, -5.1285e-01,  1.3058e-01,\n",
       "              2.3021e-01, -1.9651e-01, -1.4704e-02, -3.7934e-02, -1.0219e-01,\n",
       "             -3.8405e-03,  1.7827e-01,  5.5157e-01,  1.2217e-01, -1.6437e-01,\n",
       "             -4.1705e-03,  1.4099e-01, -1.4453e-01, -1.4905e-01, -3.8614e-01,\n",
       "              9.1981e-02, -1.4759e-01,  2.6513e-01,  4.0537e-01, -5.0670e-02,\n",
       "             -3.6645e-01,  1.0409e-01, -1.5165e-01,  3.7008e-02, -1.2164e-01,\n",
       "             -7.2685e-03, -2.3244e-01,  1.2119e-01,  1.3005e-01, -8.2041e-02],\n",
       "            [-1.7697e-01, -3.4874e-01,  2.6756e-01, -2.4898e-01, -4.8419e-01,\n",
       "              1.5101e-01,  5.2851e-02,  4.7377e-01, -4.4122e-01, -5.7937e-02,\n",
       "             -1.1078e-01, -1.4705e-02,  2.6273e-01, -1.0978e-01, -3.0852e-02,\n",
       "             -7.0195e-02,  2.7535e-01, -1.8348e-04,  2.2382e-02,  5.5757e-03,\n",
       "             -1.3357e-02, -8.9809e-02,  3.3608e-02, -1.3847e-01,  9.9673e-02,\n",
       "              4.5903e-01,  6.4790e-02,  1.5152e-01, -1.4217e-01, -2.8846e-02,\n",
       "             -7.0460e-02, -9.8426e-02,  5.0140e-02,  2.5921e-02,  2.7979e-02,\n",
       "             -8.7846e-02, -1.4008e-01,  2.2553e-02,  2.9583e-01,  1.5403e-01,\n",
       "              1.8467e-02, -1.1658e-01,  1.5332e-01,  1.5728e-02, -1.3377e-01,\n",
       "              6.0013e-01, -1.6042e-01, -5.7274e-02, -3.0960e-01, -8.4331e-02,\n",
       "             -6.5590e-02,  2.6644e-02,  2.7133e-01,  1.0369e-01, -5.0700e-01,\n",
       "              1.9658e-01,  2.6736e-01,  4.9291e-02,  1.6966e-01,  2.0430e-01,\n",
       "             -1.2142e-01, -1.9401e-01,  2.3763e-01, -3.1797e-01, -1.9029e-02,\n",
       "             -2.9049e-01,  1.2777e-02, -3.8916e-02, -4.3988e-02, -5.7324e-03,\n",
       "              1.1328e-01, -1.4088e-01, -3.6622e-02,  4.7216e-01,  5.2693e-01,\n",
       "             -4.2563e-01, -1.2959e-01,  4.9091e-01, -2.2711e-02, -3.7465e-02,\n",
       "             -2.7029e-01,  2.0266e-01, -2.7428e-01,  5.7221e-02, -1.0276e-01,\n",
       "              9.7201e-03,  8.5491e-02, -2.1519e-02,  8.6032e-02,  9.5265e-02,\n",
       "              8.0008e-02,  3.3233e-02, -4.5064e-02,  6.1759e-03,  3.9370e-02,\n",
       "             -7.0920e-02,  1.5212e-01, -3.8176e-01, -4.2366e-02,  1.9086e-01]],\n",
       "   \n",
       "           [[-5.7924e-02, -3.8901e-02,  2.2317e-02,  3.1479e-02,  4.4443e-02,\n",
       "              1.0618e-02,  4.4044e-02,  1.0729e-02, -3.5169e-02,  1.4160e-01,\n",
       "              1.0366e-01,  6.0488e-02, -8.2140e-03,  9.2704e-02, -3.1506e-02,\n",
       "             -1.1637e-01, -8.3110e-02, -1.8309e-03,  4.3611e-02,  4.4091e-02,\n",
       "              5.6292e-02,  3.6844e-02,  1.1718e-02, -1.8825e-02,  1.3503e-02,\n",
       "             -1.9313e-02, -1.9445e-02,  2.0386e-02,  8.7916e-03, -2.9219e-02,\n",
       "              2.6481e-02, -4.1261e-02,  2.4448e-02,  1.4162e-02,  7.1661e-03,\n",
       "              8.1935e-02, -3.8862e-02,  1.0484e-01,  4.7303e-02, -5.7295e-02,\n",
       "             -5.2465e-02,  1.1104e-01,  3.1687e-02,  8.8774e-02,  7.7788e-02,\n",
       "              7.9587e-03, -1.2412e-02,  8.8899e-02,  4.3076e-02,  1.7448e-02,\n",
       "             -6.2103e-02, -1.0670e-02, -5.7821e-02,  8.9059e-03,  9.1259e-02,\n",
       "              8.6555e-02, -1.0368e-01,  7.3569e-03,  1.1016e-02,  4.3283e-02,\n",
       "              2.3728e-02, -3.5126e-02,  2.9838e-02,  3.6438e-02,  3.4768e-02,\n",
       "             -4.1425e-02, -5.7040e-02,  4.3740e-02,  1.7152e-02, -7.1904e-03,\n",
       "             -7.4910e-02,  1.2103e-01, -9.6609e-02,  7.6710e-02, -7.2161e-03,\n",
       "              6.2525e-03, -4.2549e-02, -6.9283e-02, -3.8471e-02, -1.4701e-02,\n",
       "             -1.6436e-03,  3.0036e-02, -1.2776e-03, -2.6475e-02, -1.6693e-02,\n",
       "             -7.6810e-04,  2.1967e-03,  1.2875e-02,  7.6580e-03,  2.5363e-02,\n",
       "              7.4098e-02, -4.2027e-02, -3.4752e-02, -4.8202e-02,  6.7665e-02,\n",
       "             -3.7981e-02, -2.1688e-02, -3.6608e-02,  1.4730e-02,  2.9595e-02],\n",
       "            [ 9.7917e-04, -1.4478e-02, -5.1214e-02,  6.2170e-03, -6.6738e-02,\n",
       "             -1.2130e-01, -8.3803e-02, -6.3484e-02, -1.4171e-02,  1.8734e-01,\n",
       "              1.3914e-01,  3.2275e-03,  5.3791e-02,  1.9930e-03, -4.8674e-02,\n",
       "              5.5308e-02, -1.8856e-02, -1.5764e-02, -8.1696e-02, -7.2809e-02,\n",
       "              1.0005e-01,  9.4710e-02,  8.3977e-03,  5.2831e-03,  1.1118e-01,\n",
       "             -1.8883e-02,  9.0846e-02, -9.5374e-02,  2.7081e-02, -4.2348e-02,\n",
       "             -1.0531e-01, -1.0997e-01,  1.7584e-02,  8.9647e-02, -1.7585e-01,\n",
       "             -5.1127e-02,  4.7125e-02,  7.6310e-02, -2.1798e-01, -2.3981e-01,\n",
       "              1.3284e-02, -8.4489e-02, -4.4026e-02,  1.0131e-01,  4.7945e-02,\n",
       "             -1.3290e-01, -9.6732e-02,  1.3154e-01, -6.4697e-02, -7.7834e-02,\n",
       "             -1.7926e-01,  8.6738e-02, -1.6509e-02, -7.1209e-02,  7.3903e-02,\n",
       "             -1.6741e-01, -6.2202e-02,  1.8510e-01, -1.0723e-01,  4.9625e-02,\n",
       "             -3.5948e-03, -9.5924e-02, -1.1351e-01,  7.3705e-02,  1.7819e-01,\n",
       "             -4.0886e-02, -1.6832e-01,  1.0040e-01, -1.6399e-01, -1.9114e-02,\n",
       "             -4.9872e-02, -6.7707e-02, -5.2228e-03,  1.7960e-01, -8.8189e-02,\n",
       "             -8.9533e-02, -1.6713e-02, -5.4725e-04, -2.3746e-01, -7.8368e-02,\n",
       "             -4.5963e-02,  4.5573e-02, -1.0336e-01,  9.8876e-03, -2.8420e-02,\n",
       "              9.6453e-02, -5.7007e-04,  4.6651e-02,  1.4200e-01, -7.5458e-02,\n",
       "             -9.8741e-02, -1.0999e-01,  2.8799e-02, -2.3027e-02,  9.7004e-02,\n",
       "              3.7000e-02,  5.2698e-02,  1.6486e-01, -2.3309e-02,  5.0427e-02],\n",
       "            [-7.9444e-02, -7.5345e-02,  1.2638e-02,  7.9093e-02, -3.9843e-02,\n",
       "             -8.3673e-02,  2.2093e-02,  9.4155e-03, -1.4667e-01,  8.4151e-02,\n",
       "             -1.5926e-02,  1.6173e-02,  2.4244e-02,  3.0734e-03, -1.9369e-02,\n",
       "              3.0755e-02, -3.2799e-02,  3.3327e-02, -8.9183e-02,  6.7562e-02,\n",
       "              1.6020e-01, -1.1250e-02,  3.8751e-02, -6.1924e-03, -3.4610e-02,\n",
       "             -9.6256e-03,  2.1693e-02, -1.1796e-01, -3.3396e-03, -5.4959e-03,\n",
       "              2.8536e-02, -7.2127e-02,  7.7185e-02,  1.0171e-01, -4.0066e-02,\n",
       "             -7.6590e-02, -8.8235e-02,  7.4846e-02,  8.0806e-03, -5.0672e-02,\n",
       "             -3.7880e-02,  1.3196e-01,  6.5462e-02,  5.7457e-02,  4.3420e-02,\n",
       "             -1.1037e-01, -3.9625e-02,  1.8750e-01,  8.1115e-02, -6.1958e-02,\n",
       "             -7.2989e-02, -5.9156e-02, -5.4058e-02,  1.6093e-02,  6.7872e-02,\n",
       "              6.7759e-03, -1.1045e-01,  3.4546e-02, -1.1297e-01, -1.4734e-01,\n",
       "             -6.5920e-02, -4.9188e-02,  9.0562e-02, -6.1106e-02,  2.6856e-02,\n",
       "             -4.8998e-02, -1.4645e-02, -3.5295e-02,  5.9678e-02,  1.0665e-01,\n",
       "              3.7714e-02,  4.7579e-02, -1.1017e-01,  2.5934e-02,  5.8496e-02,\n",
       "             -8.6210e-02, -1.5534e-02, -7.5823e-03, -1.4011e-01,  9.8422e-02,\n",
       "              5.5937e-02, -2.8602e-02,  3.4921e-02, -9.2088e-02, -7.8865e-02,\n",
       "              3.3175e-03, -3.9254e-02, -6.2465e-02,  4.1574e-03,  1.2390e-01,\n",
       "              2.4148e-02, -4.1297e-02,  1.7305e-02, -7.1300e-02,  9.1551e-02,\n",
       "              1.1238e-01, -3.4536e-02, -6.2239e-02, -1.6873e-01, -6.0956e-04],\n",
       "            [-6.5965e-02, -1.7126e-01,  7.3300e-02,  1.0384e-03,  4.9227e-02,\n",
       "              6.6722e-02,  1.8580e-03,  2.5310e-02, -9.6312e-03,  5.0352e-02,\n",
       "              1.2054e-01,  2.6770e-02,  8.6816e-03,  6.0259e-02, -3.5288e-02,\n",
       "             -1.0941e-01, -1.0865e-01, -3.7491e-02,  1.3092e-01,  6.7724e-02,\n",
       "              1.0395e-01,  4.0167e-02, -1.2208e-02, -8.7550e-02, -1.5771e-02,\n",
       "              5.1609e-02, -5.3655e-02,  3.4291e-02,  9.9185e-02, -7.2418e-02,\n",
       "              5.8712e-02, -3.2463e-02, -4.6710e-02,  1.6895e-02,  1.3678e-03,\n",
       "              1.1114e-01,  8.1645e-02,  2.4328e-01, -9.5285e-02, -1.5429e-01,\n",
       "             -2.2218e-01, -5.6112e-02,  7.1416e-02,  8.2364e-02,  1.9160e-02,\n",
       "             -2.4850e-02, -9.8898e-02,  1.4397e-02,  1.5645e-01, -4.7958e-02,\n",
       "             -1.0420e-01, -2.4434e-02, -5.5024e-02,  1.0091e-03,  2.8685e-02,\n",
       "             -3.0008e-02, -6.5689e-02,  5.0194e-02, -6.6082e-02, -6.3758e-02,\n",
       "              8.8296e-03, -1.6892e-01,  1.1822e-01,  4.4057e-02,  7.1330e-02,\n",
       "             -2.6714e-02, -1.2822e-01,  4.0128e-02,  7.3548e-03, -4.1208e-02,\n",
       "             -4.5799e-02,  8.7943e-02,  2.8724e-03,  1.4680e-01, -1.3941e-02,\n",
       "             -1.0461e-01, -9.6526e-03,  2.6703e-02, -1.9319e-02, -1.5681e-01,\n",
       "             -1.1628e-02, -5.8146e-02, -4.2714e-03, -8.2627e-02, -2.1249e-02,\n",
       "             -7.9370e-02, -5.2884e-02,  1.2901e-02,  9.0601e-02,  7.2673e-02,\n",
       "             -8.0673e-02, -1.6917e-01, -8.7084e-03, -1.6056e-01,  6.6492e-02,\n",
       "             -5.8739e-02, -4.8674e-02, -3.0290e-02, -5.3615e-02,  1.2872e-01]]],\n",
       "          grad_fn=<StackBackward0>),\n",
       "   tensor([[[ 5.6893e-02,  3.1024e-01,  3.2424e-01,  2.0980e-01, -2.4876e-01,\n",
       "              5.6308e-02, -2.3928e-02, -3.0759e-01, -9.0790e-02, -8.1327e-02,\n",
       "              2.1734e-01,  2.0277e-03, -3.0759e-02,  2.6138e-01, -2.0473e-01,\n",
       "             -2.8426e-01,  3.1942e-01, -1.2088e-02, -1.6978e-01,  6.7131e-02,\n",
       "             -2.3850e-01,  2.1339e-01, -7.6181e-03, -3.8192e-01,  4.0990e-01,\n",
       "             -2.8603e-01,  1.0388e-01,  2.0312e-01, -2.4689e-01, -2.8741e-02,\n",
       "             -2.2498e-01,  1.7934e-02, -2.2856e-02,  2.8560e-01,  3.1520e-01,\n",
       "             -2.1638e-01,  5.4558e-03, -3.8035e-01,  4.7422e-01,  9.3980e-02,\n",
       "             -6.3140e-02, -2.2385e-01,  3.8275e-01, -1.5635e-01, -2.0105e-01,\n",
       "              5.3608e-01,  1.2497e-01, -7.2013e-03, -2.5801e-01, -3.9322e-01,\n",
       "              7.6781e-02, -6.2113e-02,  3.3234e-01, -1.1278e-01, -4.1568e-01,\n",
       "              1.5175e-01,  2.6442e-01,  2.9180e-01,  2.8030e-01,  9.9812e-02,\n",
       "              2.7712e-01, -1.8950e-01,  3.3759e-01,  1.8230e-02, -3.2486e-01,\n",
       "              3.4184e-01,  3.6243e-01,  2.5932e-01, -1.5965e-01,  3.3925e-02,\n",
       "              4.1568e-01, -2.2701e-01,  9.0008e-02, -5.9137e-02,  2.8121e-01,\n",
       "             -1.6582e-01,  1.7615e-01,  3.4835e-01, -8.6565e-02, -5.3091e-02,\n",
       "             -3.0234e-01,  3.5993e-01, -2.2088e-02,  1.7999e-01, -2.1139e-01,\n",
       "              1.4972e-01, -1.7141e-01, -1.3407e-01,  2.6278e-01, -2.3903e-01,\n",
       "             -3.9724e-02,  6.9808e-03, -2.2123e-01,  7.0549e-02,  3.1858e-01,\n",
       "              1.4232e-01, -1.0848e-01, -2.1013e-01, -1.9573e-01,  1.2356e-01],\n",
       "            [-8.2347e-01, -6.2269e-01,  2.0995e-01, -3.6484e-01,  7.4686e-02,\n",
       "              4.6606e-01,  5.2612e-01,  1.1403e-01,  2.0139e-01, -7.3393e-01,\n",
       "             -8.6280e-01, -1.5897e-01, -5.1519e-01,  7.4049e-02, -3.2895e-01,\n",
       "             -2.4694e-03,  2.1021e-01, -2.4002e-01,  3.6263e-02, -2.4474e-02,\n",
       "              6.3937e-01, -1.3244e-02,  3.3589e-02, -2.1948e-02, -6.1951e-03,\n",
       "              8.9654e-01, -5.6985e-03, -2.8947e-02, -2.2584e-01, -2.5625e-01,\n",
       "             -3.3760e-01,  7.5043e-01,  2.3393e-01, -7.7200e-01, -3.2817e-01,\n",
       "             -5.4694e-01, -9.4273e-01, -6.8054e-03, -8.1371e-01, -1.3083e-01,\n",
       "              3.6383e-02, -3.8870e-01,  3.4217e-01, -8.8435e-01,  5.6282e-02,\n",
       "              1.9134e-01,  9.1910e-01, -6.9147e-01, -2.5498e-01, -2.9472e-01,\n",
       "             -2.9291e-01,  2.8019e-04, -3.9636e-01,  8.5253e-01,  2.0036e-02,\n",
       "              4.5890e-01,  6.2492e-01, -3.4864e-01,  5.6200e-01,  4.8514e-01,\n",
       "              1.5167e-01, -2.1473e-01,  2.1184e-01,  6.3925e-02, -9.2218e-01,\n",
       "             -5.3599e-01, -7.7833e-02,  8.2475e-01,  3.9201e-01,  6.1712e-01,\n",
       "              4.2308e-01, -4.8492e-01, -1.4327e-02,  1.0123e-01,  1.0001e+00,\n",
       "              7.2853e-01, -4.9521e-01,  7.5309e-01,  2.5756e-02, -7.3115e-01,\n",
       "              1.2643e-02, -3.2665e-01,  4.6480e-01, -6.0530e-01,  3.7649e-01,\n",
       "             -6.1355e-02,  3.2110e-01,  5.3359e-02,  8.6170e-01,  2.8486e-01,\n",
       "             -6.3243e-01,  7.7809e-02, -6.2240e-02, -6.4326e-01, -4.5811e-02,\n",
       "              3.7166e-01,  9.4504e-01, -2.2637e-01, -3.9417e-01,  2.5584e-01],\n",
       "            [-4.6136e-02,  1.3937e-01,  5.8057e-01, -4.9374e-01, -2.5295e-01,\n",
       "             -9.2176e-02,  1.0228e-01,  2.5536e-01, -4.8273e-01, -4.5183e-01,\n",
       "              1.2279e-01, -5.8922e-01,  3.4918e-01,  2.9050e-01, -1.9347e-01,\n",
       "             -2.1458e-01,  6.8055e-02, -3.8886e-01,  2.8647e-02,  2.0341e-01,\n",
       "             -6.5310e-01,  8.3895e-01,  2.0034e-01, -7.7236e-01, -1.4284e-01,\n",
       "             -4.6464e-01,  4.8675e-01, -3.0415e-01, -1.7101e-01, -2.0883e-01,\n",
       "             -1.7142e-01,  8.0462e-02,  4.2683e-01,  3.9234e-01,  6.5886e-02,\n",
       "             -5.4834e-01, -5.2619e-01,  1.3839e-01, -2.7358e-01, -2.3056e-01,\n",
       "              6.1511e-02, -6.0758e-02, -6.3402e-01,  1.3515e-01,  4.5506e-01,\n",
       "              4.3408e-01,  1.1251e-01, -6.7533e-01, -8.0692e-02,  1.7567e-01,\n",
       "             -3.3917e-01, -1.9941e-01,  1.9086e-01,  1.0046e-01, -6.4422e-01,\n",
       "             -3.3078e-01,  1.5240e-01,  2.5747e-01,  4.0509e-02,  2.0283e-02,\n",
       "              6.8645e-01, -2.4662e-01,  1.7676e-01, -1.6005e-03, -3.5014e-01,\n",
       "             -1.9379e-01,  1.8186e-01,  2.1031e-02, -8.1191e-01,  2.4632e-01,\n",
       "              5.9934e-01, -2.6695e-01, -5.0402e-02, -1.8209e-01, -1.9072e-01,\n",
       "             -1.0918e-02,  3.0405e-01,  7.6514e-01,  5.2563e-01, -3.6933e-01,\n",
       "             -7.8355e-03,  1.6156e-01, -5.2754e-01, -2.0298e-01, -4.6533e-01,\n",
       "              1.4427e-01, -1.7212e-01,  4.3441e-01,  5.6449e-01, -2.2926e-01,\n",
       "             -4.7489e-01,  1.6960e-01, -2.4962e-01,  7.7090e-02, -2.4013e-01,\n",
       "             -1.3455e-02, -6.4088e-01,  4.2686e-01,  3.4538e-01, -4.2509e-01],\n",
       "            [-2.4677e-01, -4.5064e-01,  6.6144e-01, -3.6115e-01, -6.6244e-01,\n",
       "              5.1316e-01,  1.1280e-01,  6.9292e-01, -8.0707e-01, -2.9344e-01,\n",
       "             -5.5120e-01, -2.2165e-02,  3.4972e-01, -3.4880e-01, -3.9218e-02,\n",
       "             -1.0717e-01,  4.0275e-01, -2.1443e-03,  3.7653e-02,  7.5155e-03,\n",
       "             -1.5736e-02, -1.0439e-01,  1.8911e-01, -5.1989e-01,  1.4109e-01,\n",
       "              5.4877e-01,  1.5292e-01,  2.3257e-01, -2.1952e-01, -1.8801e-01,\n",
       "             -2.9976e-01, -2.3891e-01,  7.2995e-02,  1.6716e-01,  6.7503e-02,\n",
       "             -6.2042e-01, -8.0009e-01,  8.7442e-02,  5.2998e-01,  3.6479e-01,\n",
       "              5.6833e-02, -2.0126e-01,  3.3875e-01,  3.5751e-02, -2.3875e-01,\n",
       "              8.9754e-01, -4.8197e-01, -7.7044e-02, -5.1299e-01, -3.1515e-01,\n",
       "             -1.1438e-01,  8.0766e-02,  4.4926e-01,  4.0758e-01, -7.8390e-01,\n",
       "              3.1398e-01,  4.2493e-01,  1.6943e-01,  3.1954e-01,  4.2433e-01,\n",
       "             -3.5565e-01, -4.0293e-01,  2.9141e-01, -6.1765e-01, -9.0750e-02,\n",
       "             -5.5488e-01,  2.5134e-01, -4.5698e-02, -5.2398e-02, -2.6149e-02,\n",
       "              3.4371e-01, -3.5212e-01, -6.0123e-02,  8.5248e-01,  7.6375e-01,\n",
       "             -7.5326e-01, -4.8785e-01,  7.2016e-01, -5.1519e-02, -1.3071e-01,\n",
       "             -3.7810e-01,  2.4506e-01, -6.4572e-01,  6.9149e-02, -1.5819e-01,\n",
       "              1.1724e-02,  2.1796e-01, -4.1528e-02,  3.4150e-01,  1.6689e-01,\n",
       "              2.4903e-01,  4.3805e-02, -9.4956e-02,  9.7050e-03,  2.1572e-01,\n",
       "             -2.5510e-01,  4.5345e-01, -6.5277e-01, -1.6366e-01,  2.4813e-01]],\n",
       "   \n",
       "           [[-1.1476e-01, -7.1236e-02,  5.6456e-02,  6.6514e-02,  7.9718e-02,\n",
       "              1.9897e-02,  9.0827e-02,  2.5960e-02, -7.1369e-02,  3.0391e-01,\n",
       "              1.8914e-01,  1.4455e-01, -1.8264e-02,  1.8090e-01, -7.2514e-02,\n",
       "             -2.1584e-01, -1.4272e-01, -3.5080e-03,  9.2361e-02,  1.0228e-01,\n",
       "              1.1034e-01,  6.0595e-02,  2.1436e-02, -3.6957e-02,  2.9814e-02,\n",
       "             -4.6162e-02, -3.6321e-02,  3.8040e-02,  1.5373e-02, -6.2899e-02,\n",
       "              4.6816e-02, -9.1425e-02,  6.7017e-02,  2.7831e-02,  1.8226e-02,\n",
       "              1.6294e-01, -9.3975e-02,  1.7457e-01,  9.9020e-02, -1.0338e-01,\n",
       "             -9.5263e-02,  2.2706e-01,  6.0956e-02,  1.5108e-01,  1.6343e-01,\n",
       "              1.5625e-02, -2.3392e-02,  1.6363e-01,  9.8094e-02,  3.7154e-02,\n",
       "             -1.1368e-01, -2.2873e-02, -1.3517e-01,  2.0014e-02,  1.6883e-01,\n",
       "              1.5827e-01, -2.3457e-01,  1.7292e-02,  2.7095e-02,  8.0111e-02,\n",
       "              4.5510e-02, -6.7909e-02,  6.0789e-02,  6.8839e-02,  8.2626e-02,\n",
       "             -7.2290e-02, -1.2878e-01,  9.6424e-02,  3.0747e-02, -1.4750e-02,\n",
       "             -1.2766e-01,  2.2826e-01, -1.7803e-01,  1.6949e-01, -1.3634e-02,\n",
       "              1.3023e-02, -8.3407e-02, -1.1577e-01, -8.0733e-02, -3.9181e-02,\n",
       "             -3.7716e-03,  6.3216e-02, -3.0346e-03, -5.5695e-02, -2.6933e-02,\n",
       "             -1.6403e-03,  4.4115e-03,  2.4830e-02,  1.5630e-02,  4.5140e-02,\n",
       "              1.4814e-01, -7.3078e-02, -8.0321e-02, -8.4246e-02,  1.7734e-01,\n",
       "             -7.8750e-02, -4.1711e-02, -6.9715e-02,  2.6591e-02,  7.0739e-02],\n",
       "            [ 2.7820e-03, -2.4352e-02, -9.9350e-02,  1.5552e-02, -9.6855e-02,\n",
       "             -2.0841e-01, -1.6808e-01, -1.3112e-01, -3.5101e-02,  2.8294e-01,\n",
       "              4.5633e-01,  6.5630e-03,  9.0915e-02,  4.2622e-03, -1.2370e-01,\n",
       "              8.6541e-02, -8.7512e-02, -3.4620e-02, -1.8554e-01, -1.2215e-01,\n",
       "              1.9315e-01,  1.8687e-01,  1.2518e-02,  1.0994e-02,  1.9975e-01,\n",
       "             -4.5376e-02,  1.4827e-01, -1.6052e-01,  6.5714e-02, -9.0615e-02,\n",
       "             -2.3138e-01, -2.2692e-01,  4.1623e-02,  1.9716e-01, -3.8338e-01,\n",
       "             -9.4659e-02,  7.5827e-02,  1.4923e-01, -4.3617e-01, -4.4275e-01,\n",
       "              2.4738e-02, -2.1298e-01, -7.2983e-02,  2.3397e-01,  9.9468e-02,\n",
       "             -2.5275e-01, -2.2937e-01,  2.4912e-01, -1.0883e-01, -1.5657e-01,\n",
       "             -3.2758e-01,  1.1274e-01, -3.4660e-02, -1.5538e-01,  1.4350e-01,\n",
       "             -3.4495e-01, -1.2968e-01,  3.8280e-01, -2.2809e-01,  8.2027e-02,\n",
       "             -6.3954e-03, -1.8199e-01, -1.6790e-01,  2.3037e-01,  4.0602e-01,\n",
       "             -1.1491e-01, -3.8399e-01,  2.0437e-01, -2.7758e-01, -2.9976e-02,\n",
       "             -9.9126e-02, -1.3659e-01, -1.1226e-02,  3.4109e-01, -2.4425e-01,\n",
       "             -1.2619e-01, -4.1533e-02, -1.4421e-03, -4.4312e-01, -1.8510e-01,\n",
       "             -9.9847e-02,  9.5612e-02, -1.9833e-01,  2.1740e-02, -8.7885e-02,\n",
       "              2.3772e-01, -1.0210e-03,  9.4076e-02,  2.9923e-01, -1.4366e-01,\n",
       "             -1.6378e-01, -1.9476e-01,  5.2312e-02, -4.1764e-02,  1.5564e-01,\n",
       "              1.2695e-01,  9.7717e-02,  3.0273e-01, -4.7146e-02,  1.3594e-01],\n",
       "            [-1.2569e-01, -1.3303e-01,  3.4267e-02,  1.5234e-01, -8.3178e-02,\n",
       "             -1.7507e-01,  4.7634e-02,  2.7732e-02, -3.1525e-01,  1.6144e-01,\n",
       "             -3.2546e-02,  4.0738e-02,  5.8061e-02,  6.3530e-03, -3.4697e-02,\n",
       "              5.5895e-02, -6.1951e-02,  5.9834e-02, -1.5674e-01,  1.2739e-01,\n",
       "              3.1292e-01, -2.3758e-02,  6.7920e-02, -1.5199e-02, -6.7394e-02,\n",
       "             -2.1120e-02,  4.1792e-02, -2.1813e-01, -5.2378e-03, -1.1507e-02,\n",
       "              7.8199e-02, -1.8848e-01,  1.9637e-01,  1.8070e-01, -1.2682e-01,\n",
       "             -1.9035e-01, -2.8460e-01,  1.8125e-01,  1.8381e-02, -9.6266e-02,\n",
       "             -7.4808e-02,  3.0753e-01,  1.1980e-01,  1.2142e-01,  9.7056e-02,\n",
       "             -2.0813e-01, -8.4460e-02,  3.1481e-01,  1.9654e-01, -1.5521e-01,\n",
       "             -1.2797e-01, -1.2354e-01, -9.9376e-02,  2.7829e-02,  1.2927e-01,\n",
       "              1.7332e-02, -2.8032e-01,  5.6012e-02, -2.5825e-01, -2.6418e-01,\n",
       "             -1.3572e-01, -8.5275e-02,  2.4725e-01, -1.3734e-01,  6.3796e-02,\n",
       "             -7.8284e-02, -3.3230e-02, -6.6138e-02,  1.2292e-01,  1.9545e-01,\n",
       "              7.5563e-02,  1.0790e-01, -2.0365e-01,  7.8106e-02,  1.1661e-01,\n",
       "             -2.1506e-01, -3.7859e-02, -1.2239e-02, -3.1542e-01,  2.8089e-01,\n",
       "              1.2802e-01, -4.9211e-02,  7.2535e-02, -1.8648e-01, -1.5288e-01,\n",
       "              8.5757e-03, -7.8250e-02, -1.3742e-01,  7.6672e-03,  2.3930e-01,\n",
       "              5.6924e-02, -6.7476e-02,  3.2003e-02, -1.5130e-01,  1.7845e-01,\n",
       "              2.4236e-01, -7.7603e-02, -1.1842e-01, -2.9027e-01, -1.3640e-03],\n",
       "            [-2.0154e-01, -2.8393e-01,  1.5132e-01,  2.8261e-03,  8.5190e-02,\n",
       "              1.2041e-01,  3.6218e-03,  5.6213e-02, -1.6084e-02,  9.3049e-02,\n",
       "              3.0071e-01,  8.2119e-02,  1.7006e-02,  1.7682e-01, -8.8993e-02,\n",
       "             -1.8878e-01, -1.9079e-01, -5.8787e-02,  2.7069e-01,  1.1316e-01,\n",
       "              2.3356e-01,  7.0733e-02, -2.2127e-02, -2.0050e-01, -3.0199e-02,\n",
       "              1.2641e-01, -9.3637e-02,  6.6825e-02,  1.9949e-01, -1.3644e-01,\n",
       "              1.1554e-01, -6.4291e-02, -1.0528e-01,  3.7878e-02,  3.4421e-03,\n",
       "              2.3544e-01,  1.5153e-01,  4.6874e-01, -1.5047e-01, -2.9126e-01,\n",
       "             -4.4420e-01, -1.0534e-01,  1.3436e-01,  1.4306e-01,  4.4037e-02,\n",
       "             -5.4017e-02, -1.9314e-01,  2.6430e-02,  3.7109e-01, -1.0518e-01,\n",
       "             -1.7362e-01, -5.6570e-02, -1.1766e-01,  2.4541e-03,  6.0822e-02,\n",
       "             -5.4278e-02, -1.2890e-01,  1.1472e-01, -1.6344e-01, -1.0667e-01,\n",
       "              1.5234e-02, -3.6515e-01,  1.8780e-01,  1.1436e-01,  1.7649e-01,\n",
       "             -5.5914e-02, -2.8267e-01,  1.3264e-01,  1.3761e-02, -7.5867e-02,\n",
       "             -6.6127e-02,  1.6352e-01,  4.3740e-03,  2.4710e-01, -3.0027e-02,\n",
       "             -1.8950e-01, -1.6580e-02,  4.0689e-02, -3.7960e-02, -2.8244e-01,\n",
       "             -2.2497e-02, -1.2666e-01, -1.3282e-02, -1.7578e-01, -4.9075e-02,\n",
       "             -1.5097e-01, -1.0839e-01,  2.6159e-02,  1.4740e-01,  1.1630e-01,\n",
       "             -1.3078e-01, -2.6590e-01, -1.5904e-02, -2.6519e-01,  1.2421e-01,\n",
       "             -1.2099e-01, -8.5827e-02, -6.2796e-02, -7.3944e-02,  3.3275e-01]]],\n",
       "          grad_fn=<StackBackward0>)),\n",
       "  [tensor([[0.0087, 0.0086, 0.0085, 0.0080, 0.0072, 0.0079, 0.0086, 0.0096, 0.0093,\n",
       "            0.0088, 0.0077, 0.0079, 0.0092, 0.0099, 0.0090, 0.0080, 0.0080, 0.0084,\n",
       "            0.0084, 0.0082, 0.0082, 0.0082, 0.0084, 0.0078, 0.0072, 0.0073, 0.0077,\n",
       "            0.0079, 0.0078, 0.0081, 0.0086, 0.0088, 0.0085, 0.0082, 0.0080, 0.0081,\n",
       "            0.0083, 0.0079, 0.0076, 0.0075, 0.0079, 0.0081, 0.0078, 0.0075, 0.0083,\n",
       "            0.0082, 0.0082, 0.0077, 0.0088, 0.0091, 0.0088, 0.0082, 0.0081, 0.0080,\n",
       "            0.0082, 0.0084, 0.0085, 0.0083, 0.0093, 0.0097, 0.0101, 0.0089, 0.0092,\n",
       "            0.0089, 0.0092, 0.0085, 0.0078, 0.0084, 0.0083, 0.0093, 0.0087, 0.0084,\n",
       "            0.0084, 0.0085, 0.0089, 0.0087, 0.0077, 0.0077, 0.0076, 0.0079, 0.0078,\n",
       "            0.0079, 0.0092, 0.0096, 0.0096, 0.0084, 0.0073, 0.0070, 0.0072, 0.0080,\n",
       "            0.0078, 0.0078, 0.0078, 0.0080, 0.0082, 0.0076, 0.0080, 0.0075, 0.0082,\n",
       "            0.0081, 0.0085, 0.0084, 0.0084, 0.0082, 0.0082, 0.0082, 0.0085, 0.0083,\n",
       "            0.0087, 0.0091, 0.0098, 0.0094, 0.0086, 0.0082, 0.0080, 0.0081, 0.0082,\n",
       "            0.0083, 0.0089, 0.0087],\n",
       "           [0.0090, 0.0088, 0.0085, 0.0077, 0.0077, 0.0076, 0.0081, 0.0084, 0.0089,\n",
       "            0.0082, 0.0078, 0.0080, 0.0082, 0.0085, 0.0085, 0.0088, 0.0083, 0.0090,\n",
       "            0.0090, 0.0091, 0.0078, 0.0079, 0.0082, 0.0084, 0.0081, 0.0081, 0.0075,\n",
       "            0.0071, 0.0076, 0.0082, 0.0091, 0.0091, 0.0089, 0.0087, 0.0084, 0.0091,\n",
       "            0.0090, 0.0086, 0.0076, 0.0079, 0.0079, 0.0088, 0.0086, 0.0092, 0.0087,\n",
       "            0.0092, 0.0088, 0.0086, 0.0083, 0.0084, 0.0077, 0.0073, 0.0079, 0.0087,\n",
       "            0.0091, 0.0081, 0.0082, 0.0078, 0.0083, 0.0082, 0.0088, 0.0085, 0.0081,\n",
       "            0.0080, 0.0076, 0.0072, 0.0068, 0.0079, 0.0083, 0.0082, 0.0077, 0.0081,\n",
       "            0.0087, 0.0090, 0.0086, 0.0084, 0.0076, 0.0073, 0.0081, 0.0091, 0.0092,\n",
       "            0.0085, 0.0085, 0.0091, 0.0098, 0.0096, 0.0089, 0.0081, 0.0070, 0.0078,\n",
       "            0.0082, 0.0091, 0.0083, 0.0081, 0.0078, 0.0079, 0.0078, 0.0082, 0.0081,\n",
       "            0.0084, 0.0082, 0.0081, 0.0080, 0.0078, 0.0078, 0.0075, 0.0084, 0.0087,\n",
       "            0.0091, 0.0086, 0.0086, 0.0089, 0.0087, 0.0086, 0.0080, 0.0083, 0.0090,\n",
       "            0.0091, 0.0089, 0.0083],\n",
       "           [0.0092, 0.0085, 0.0087, 0.0085, 0.0088, 0.0091, 0.0092, 0.0094, 0.0087,\n",
       "            0.0082, 0.0069, 0.0074, 0.0079, 0.0084, 0.0083, 0.0086, 0.0088, 0.0084,\n",
       "            0.0077, 0.0074, 0.0073, 0.0080, 0.0081, 0.0077, 0.0077, 0.0079, 0.0084,\n",
       "            0.0082, 0.0085, 0.0083, 0.0085, 0.0081, 0.0087, 0.0081, 0.0082, 0.0079,\n",
       "            0.0087, 0.0091, 0.0095, 0.0090, 0.0089, 0.0086, 0.0091, 0.0083, 0.0084,\n",
       "            0.0078, 0.0079, 0.0078, 0.0082, 0.0081, 0.0079, 0.0077, 0.0079, 0.0078,\n",
       "            0.0080, 0.0076, 0.0077, 0.0076, 0.0082, 0.0079, 0.0086, 0.0081, 0.0083,\n",
       "            0.0077, 0.0078, 0.0085, 0.0084, 0.0087, 0.0078, 0.0080, 0.0077, 0.0078,\n",
       "            0.0083, 0.0083, 0.0085, 0.0084, 0.0080, 0.0078, 0.0082, 0.0090, 0.0090,\n",
       "            0.0089, 0.0096, 0.0098, 0.0091, 0.0087, 0.0079, 0.0076, 0.0071, 0.0079,\n",
       "            0.0078, 0.0082, 0.0087, 0.0093, 0.0089, 0.0083, 0.0084, 0.0091, 0.0084,\n",
       "            0.0079, 0.0075, 0.0075, 0.0076, 0.0076, 0.0084, 0.0087, 0.0095, 0.0093,\n",
       "            0.0093, 0.0093, 0.0094, 0.0095, 0.0089, 0.0089, 0.0080, 0.0073, 0.0070,\n",
       "            0.0080, 0.0091, 0.0094],\n",
       "           [0.0080, 0.0077, 0.0080, 0.0078, 0.0075, 0.0081, 0.0090, 0.0099, 0.0094,\n",
       "            0.0086, 0.0078, 0.0078, 0.0082, 0.0089, 0.0085, 0.0080, 0.0075, 0.0081,\n",
       "            0.0088, 0.0088, 0.0086, 0.0089, 0.0091, 0.0082, 0.0074, 0.0072, 0.0073,\n",
       "            0.0068, 0.0069, 0.0073, 0.0083, 0.0087, 0.0083, 0.0081, 0.0076, 0.0076,\n",
       "            0.0079, 0.0086, 0.0083, 0.0078, 0.0073, 0.0073, 0.0078, 0.0077, 0.0086,\n",
       "            0.0087, 0.0087, 0.0082, 0.0084, 0.0085, 0.0080, 0.0076, 0.0080, 0.0084,\n",
       "            0.0088, 0.0084, 0.0087, 0.0088, 0.0099, 0.0096, 0.0101, 0.0090, 0.0094,\n",
       "            0.0089, 0.0094, 0.0089, 0.0080, 0.0085, 0.0083, 0.0089, 0.0084, 0.0087,\n",
       "            0.0094, 0.0103, 0.0100, 0.0094, 0.0076, 0.0071, 0.0072, 0.0077, 0.0079,\n",
       "            0.0078, 0.0083, 0.0086, 0.0092, 0.0087, 0.0079, 0.0071, 0.0073, 0.0080,\n",
       "            0.0083, 0.0089, 0.0086, 0.0084, 0.0080, 0.0076, 0.0078, 0.0080, 0.0083,\n",
       "            0.0081, 0.0082, 0.0091, 0.0098, 0.0095, 0.0083, 0.0077, 0.0081, 0.0082,\n",
       "            0.0084, 0.0084, 0.0086, 0.0087, 0.0084, 0.0082, 0.0083, 0.0081, 0.0081,\n",
       "            0.0083, 0.0088, 0.0085]], grad_fn=<DivBackward0>),\n",
       "   tensor([[0.0097, 0.0097, 0.0090, 0.0091, 0.0088, 0.0095, 0.0089, 0.0087, 0.0078,\n",
       "            0.0078, 0.0075, 0.0083, 0.0083, 0.0084, 0.0082, 0.0077, 0.0073, 0.0072,\n",
       "            0.0082, 0.0089, 0.0090, 0.0084, 0.0085, 0.0083, 0.0083, 0.0084, 0.0080,\n",
       "            0.0084, 0.0081, 0.0084, 0.0079, 0.0080, 0.0081, 0.0086, 0.0087, 0.0087,\n",
       "            0.0087, 0.0086, 0.0086, 0.0084, 0.0082, 0.0085, 0.0088, 0.0086, 0.0082,\n",
       "            0.0079, 0.0081, 0.0081, 0.0085, 0.0087, 0.0089, 0.0083, 0.0081, 0.0077,\n",
       "            0.0079, 0.0082, 0.0082, 0.0080, 0.0077, 0.0080, 0.0080, 0.0073, 0.0075,\n",
       "            0.0077, 0.0085, 0.0086, 0.0088, 0.0085, 0.0085, 0.0091, 0.0099, 0.0092,\n",
       "            0.0086, 0.0079, 0.0083, 0.0082, 0.0085, 0.0081, 0.0082, 0.0081, 0.0084,\n",
       "            0.0082, 0.0082, 0.0084, 0.0083, 0.0080, 0.0077, 0.0073, 0.0076, 0.0074,\n",
       "            0.0079, 0.0080, 0.0085, 0.0084, 0.0083, 0.0085, 0.0083, 0.0080, 0.0077,\n",
       "            0.0078, 0.0076, 0.0076, 0.0081, 0.0085, 0.0092, 0.0089, 0.0085, 0.0076,\n",
       "            0.0083, 0.0089, 0.0088, 0.0086, 0.0083, 0.0087, 0.0079, 0.0080, 0.0080,\n",
       "            0.0089, 0.0096, 0.0100],\n",
       "           [0.0094, 0.0092, 0.0090, 0.0086, 0.0083, 0.0085, 0.0084, 0.0087, 0.0081,\n",
       "            0.0082, 0.0078, 0.0086, 0.0088, 0.0087, 0.0079, 0.0081, 0.0081, 0.0081,\n",
       "            0.0082, 0.0086, 0.0090, 0.0086, 0.0084, 0.0081, 0.0082, 0.0085, 0.0086,\n",
       "            0.0089, 0.0089, 0.0088, 0.0084, 0.0084, 0.0084, 0.0087, 0.0089, 0.0085,\n",
       "            0.0086, 0.0083, 0.0085, 0.0081, 0.0079, 0.0082, 0.0083, 0.0084, 0.0078,\n",
       "            0.0077, 0.0078, 0.0082, 0.0081, 0.0086, 0.0091, 0.0089, 0.0085, 0.0084,\n",
       "            0.0088, 0.0088, 0.0083, 0.0078, 0.0076, 0.0081, 0.0082, 0.0082, 0.0080,\n",
       "            0.0080, 0.0084, 0.0082, 0.0080, 0.0073, 0.0080, 0.0083, 0.0091, 0.0087,\n",
       "            0.0084, 0.0084, 0.0083, 0.0085, 0.0081, 0.0083, 0.0080, 0.0088, 0.0087,\n",
       "            0.0087, 0.0079, 0.0080, 0.0081, 0.0083, 0.0083, 0.0083, 0.0079, 0.0077,\n",
       "            0.0080, 0.0082, 0.0083, 0.0080, 0.0079, 0.0079, 0.0081, 0.0078, 0.0079,\n",
       "            0.0081, 0.0087, 0.0086, 0.0084, 0.0082, 0.0088, 0.0086, 0.0084, 0.0075,\n",
       "            0.0084, 0.0085, 0.0084, 0.0081, 0.0084, 0.0086, 0.0081, 0.0081, 0.0083,\n",
       "            0.0083, 0.0085, 0.0087],\n",
       "           [0.0096, 0.0096, 0.0089, 0.0092, 0.0091, 0.0095, 0.0090, 0.0085, 0.0075,\n",
       "            0.0074, 0.0077, 0.0089, 0.0086, 0.0083, 0.0076, 0.0081, 0.0078, 0.0076,\n",
       "            0.0078, 0.0081, 0.0082, 0.0082, 0.0084, 0.0084, 0.0083, 0.0085, 0.0084,\n",
       "            0.0086, 0.0090, 0.0090, 0.0085, 0.0084, 0.0083, 0.0088, 0.0084, 0.0081,\n",
       "            0.0083, 0.0084, 0.0085, 0.0077, 0.0077, 0.0082, 0.0089, 0.0083, 0.0082,\n",
       "            0.0084, 0.0089, 0.0086, 0.0088, 0.0089, 0.0098, 0.0088, 0.0087, 0.0080,\n",
       "            0.0084, 0.0081, 0.0082, 0.0079, 0.0076, 0.0078, 0.0078, 0.0078, 0.0079,\n",
       "            0.0084, 0.0088, 0.0087, 0.0089, 0.0090, 0.0093, 0.0091, 0.0100, 0.0094,\n",
       "            0.0087, 0.0082, 0.0081, 0.0083, 0.0085, 0.0083, 0.0082, 0.0078, 0.0078,\n",
       "            0.0080, 0.0079, 0.0082, 0.0084, 0.0087, 0.0090, 0.0085, 0.0083, 0.0076,\n",
       "            0.0076, 0.0076, 0.0083, 0.0083, 0.0078, 0.0073, 0.0071, 0.0075, 0.0074,\n",
       "            0.0075, 0.0082, 0.0084, 0.0087, 0.0087, 0.0090, 0.0082, 0.0078, 0.0070,\n",
       "            0.0079, 0.0080, 0.0079, 0.0081, 0.0075, 0.0077, 0.0074, 0.0080, 0.0086,\n",
       "            0.0086, 0.0090, 0.0095],\n",
       "           [0.0098, 0.0100, 0.0094, 0.0088, 0.0082, 0.0087, 0.0084, 0.0088, 0.0079,\n",
       "            0.0082, 0.0077, 0.0085, 0.0086, 0.0088, 0.0087, 0.0078, 0.0076, 0.0074,\n",
       "            0.0084, 0.0089, 0.0092, 0.0086, 0.0082, 0.0078, 0.0079, 0.0082, 0.0084,\n",
       "            0.0087, 0.0084, 0.0085, 0.0079, 0.0081, 0.0083, 0.0089, 0.0093, 0.0091,\n",
       "            0.0090, 0.0083, 0.0077, 0.0075, 0.0080, 0.0086, 0.0086, 0.0083, 0.0077,\n",
       "            0.0072, 0.0071, 0.0075, 0.0082, 0.0088, 0.0090, 0.0086, 0.0080, 0.0077,\n",
       "            0.0085, 0.0089, 0.0086, 0.0077, 0.0078, 0.0080, 0.0080, 0.0075, 0.0078,\n",
       "            0.0078, 0.0082, 0.0087, 0.0085, 0.0084, 0.0084, 0.0094, 0.0095, 0.0085,\n",
       "            0.0079, 0.0081, 0.0086, 0.0089, 0.0087, 0.0084, 0.0083, 0.0085, 0.0087,\n",
       "            0.0086, 0.0086, 0.0090, 0.0087, 0.0085, 0.0084, 0.0081, 0.0079, 0.0073,\n",
       "            0.0076, 0.0080, 0.0087, 0.0087, 0.0082, 0.0079, 0.0077, 0.0076, 0.0079,\n",
       "            0.0082, 0.0080, 0.0079, 0.0084, 0.0088, 0.0092, 0.0088, 0.0084, 0.0075,\n",
       "            0.0079, 0.0083, 0.0082, 0.0076, 0.0079, 0.0084, 0.0083, 0.0083, 0.0086,\n",
       "            0.0086, 0.0086, 0.0090]], grad_fn=<DivBackward0>)]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first_char = first_char.unsqueeze(0)\n",
    "o, prev_state = cell(first_char)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0300, -0.0147,  0.0086,  ...,  0.0491, -0.0213,  0.0236],\n",
       "         [ 0.0414,  0.0359,  0.0154,  ...,  0.0299, -0.0052,  0.0160],\n",
       "         [-0.0052,  0.0181, -0.0204,  ...,  0.0037, -0.0510, -0.0168],\n",
       "         ...,\n",
       "         [ 0.0456, -0.0360,  0.0263,  ...,  0.0099,  0.0357, -0.0155],\n",
       "         [-0.0272, -0.0205, -0.0326,  ...,  0.0164, -0.0600, -0.0427],\n",
       "         [ 0.0591,  0.0145, -0.0403,  ..., -0.0066, -0.0461,  0.0003]],\n",
       "\n",
       "        [[-0.0291, -0.0160,  0.0087,  ...,  0.0499, -0.0207,  0.0235],\n",
       "         [ 0.0423,  0.0346,  0.0155,  ...,  0.0306, -0.0047,  0.0160],\n",
       "         [-0.0044,  0.0169, -0.0203,  ...,  0.0044, -0.0504, -0.0169],\n",
       "         ...,\n",
       "         [ 0.0465, -0.0372,  0.0264,  ...,  0.0105,  0.0362, -0.0155],\n",
       "         [-0.0263, -0.0218, -0.0326,  ...,  0.0171, -0.0596, -0.0428],\n",
       "         [ 0.0601,  0.0133, -0.0403,  ..., -0.0059, -0.0457,  0.0003]],\n",
       "\n",
       "        [[-0.0294, -0.0148,  0.0090,  ...,  0.0486, -0.0210,  0.0241],\n",
       "         [ 0.0420,  0.0358,  0.0158,  ...,  0.0294, -0.0050,  0.0166],\n",
       "         [-0.0047,  0.0180, -0.0200,  ...,  0.0032, -0.0508, -0.0163],\n",
       "         ...,\n",
       "         [ 0.0462, -0.0361,  0.0267,  ...,  0.0094,  0.0359, -0.0150],\n",
       "         [-0.0266, -0.0206, -0.0323,  ...,  0.0160, -0.0598, -0.0422],\n",
       "         [ 0.0597,  0.0144, -0.0399,  ..., -0.0070, -0.0459,  0.0009]],\n",
       "\n",
       "        [[-0.0291, -0.0148,  0.0089,  ...,  0.0492, -0.0215,  0.0240],\n",
       "         [ 0.0423,  0.0359,  0.0157,  ...,  0.0299, -0.0054,  0.0165],\n",
       "         [-0.0044,  0.0181, -0.0201,  ...,  0.0037, -0.0512, -0.0164],\n",
       "         ...,\n",
       "         [ 0.0465, -0.0361,  0.0265,  ...,  0.0099,  0.0355, -0.0151],\n",
       "         [-0.0263, -0.0207, -0.0325,  ...,  0.0165, -0.0603, -0.0424],\n",
       "         [ 0.0600,  0.0144, -0.0401,  ..., -0.0065, -0.0464,  0.0007]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cell.memory.memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "char2 = features_ndarray[4:8]\n",
    "char2 = torch.from_numpy(char2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell(char2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-3.1923e-02, -1.4357e-02,  9.1393e-03,  ...,  4.8428e-02,\n",
       "          -2.0301e-02,  2.3230e-02],\n",
       "         [ 3.9108e-02,  3.6065e-02,  1.5936e-02,  ...,  2.9256e-02,\n",
       "          -4.2795e-03,  1.5707e-02],\n",
       "         [-7.2628e-03,  1.8332e-02, -1.9701e-02,  ...,  3.1880e-03,\n",
       "          -4.9884e-02, -1.6962e-02],\n",
       "         ...,\n",
       "         [ 4.3497e-02, -3.5607e-02,  2.6737e-02,  ...,  9.3972e-03,\n",
       "           3.6376e-02, -1.5626e-02],\n",
       "         [-2.9071e-02, -2.0159e-02, -3.1908e-02,  ...,  1.5878e-02,\n",
       "          -5.8813e-02, -4.2741e-02],\n",
       "         [ 5.6733e-02,  1.4767e-02, -3.9490e-02,  ..., -6.9856e-03,\n",
       "          -4.4977e-02,  1.1068e-04]],\n",
       "\n",
       "        [[-2.9835e-02, -1.6253e-02,  9.0972e-03,  ...,  4.9763e-02,\n",
       "          -1.9108e-02,  2.3110e-02],\n",
       "         [ 4.1273e-02,  3.4162e-02,  1.5883e-02,  ...,  3.0591e-02,\n",
       "          -3.1152e-03,  1.5589e-02],\n",
       "         [-5.1535e-03,  1.6510e-02, -1.9785e-02,  ...,  4.4725e-03,\n",
       "          -4.8800e-02, -1.7060e-02],\n",
       "         ...,\n",
       "         [ 4.5616e-02, -3.7333e-02,  2.6656e-02,  ...,  1.0599e-02,\n",
       "           3.7300e-02, -1.5720e-02],\n",
       "         [-2.6912e-02, -2.1955e-02, -3.2094e-02,  ...,  1.7135e-02,\n",
       "          -5.8058e-02, -4.2875e-02],\n",
       "         [ 5.9013e-02,  1.2925e-02, -3.9656e-02,  ..., -5.7180e-03,\n",
       "          -4.4086e-02,  1.4840e-07]],\n",
       "\n",
       "        [[-2.8075e-02, -1.3842e-02,  9.5462e-03,  ...,  4.7697e-02,\n",
       "          -2.0125e-02,  2.4326e-02],\n",
       "         [ 4.2975e-02,  3.6537e-02,  1.6326e-02,  ...,  2.8556e-02,\n",
       "          -4.1189e-03,  1.6814e-02],\n",
       "         [-3.5291e-03,  1.8769e-02, -1.9378e-02,  ...,  2.5715e-03,\n",
       "          -4.9852e-02, -1.5928e-02],\n",
       "         ...,\n",
       "         [ 4.7020e-02, -3.5228e-02,  2.7051e-02,  ...,  8.8257e-03,\n",
       "           3.6487e-02, -1.4655e-02],\n",
       "         [-2.5387e-02, -1.9780e-02, -3.1625e-02,  ...,  1.5288e-02,\n",
       "          -5.8881e-02, -4.1749e-02],\n",
       "         [ 6.0576e-02,  1.5199e-02, -3.9152e-02,  ..., -7.6401e-03,\n",
       "          -4.4954e-02,  1.1848e-03]],\n",
       "\n",
       "        [[-2.8061e-02, -1.4009e-02,  8.9793e-03,  ...,  4.8733e-02,\n",
       "          -2.0477e-02,  2.4931e-02],\n",
       "         [ 4.3076e-02,  3.6422e-02,  1.5776e-02,  ...,  2.9567e-02,\n",
       "          -4.4249e-03,  1.7470e-02],\n",
       "         [-3.4403e-03,  1.8675e-02, -1.9868e-02,  ...,  3.5150e-03,\n",
       "          -5.0048e-02, -1.5272e-02],\n",
       "         ...,\n",
       "         [ 4.7091e-02, -3.5383e-02,  2.6559e-02,  ...,  9.7291e-03,\n",
       "           3.6116e-02, -1.4154e-02],\n",
       "         [-2.5407e-02, -1.9994e-02, -3.2194e-02,  ...,  1.6249e-02,\n",
       "          -5.9302e-02, -4.1312e-02],\n",
       "         [ 6.0646e-02,  1.5001e-02, -3.9763e-02,  ..., -6.6342e-03,\n",
       "          -4.5397e-02,  1.6789e-03]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cell.memory.memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "char3 = features_ndarray[8:12]\n",
    "char3 = torch.from_numpy(char3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/ashatya/Data/work/envs/torch/lib/python3.10/site-packages/torch/nn/functional.py:1967: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-3.5559e-02, -1.3045e-02,  9.0926e-03,  ...,  4.6945e-02,\n",
       "          -1.8037e-02,  2.3270e-02],\n",
       "         [ 3.4572e-02,  3.7036e-02,  1.5803e-02,  ...,  2.7877e-02,\n",
       "          -2.0244e-03,  1.5848e-02],\n",
       "         [-1.1400e-02,  1.9469e-02, -1.9446e-02,  ...,  2.0839e-03,\n",
       "          -4.7133e-02, -1.6467e-02],\n",
       "         ...,\n",
       "         [ 3.9251e-02, -3.4127e-02,  2.6501e-02,  ...,  8.3167e-03,\n",
       "           3.8085e-02, -1.5181e-02],\n",
       "         [-3.2769e-02, -1.8779e-02, -3.1538e-02,  ...,  1.4713e-02,\n",
       "          -5.6145e-02, -4.2031e-02],\n",
       "         [ 5.2277e-02,  1.5855e-02, -3.9039e-02,  ..., -7.9036e-03,\n",
       "          -4.2453e-02,  3.8500e-04]],\n",
       "\n",
       "        [[-3.3219e-02, -1.7239e-02,  1.2133e-02,  ...,  4.8540e-02,\n",
       "          -1.6487e-02,  2.2783e-02],\n",
       "         [ 3.7045e-02,  3.2605e-02,  1.8903e-02,  ...,  2.9549e-02,\n",
       "          -6.1213e-04,  1.5332e-02],\n",
       "         [-8.5018e-03,  1.5285e-02, -1.6700e-02,  ...,  3.7910e-03,\n",
       "          -4.6107e-02, -1.6991e-02],\n",
       "         ...,\n",
       "         [ 4.2496e-02, -3.7894e-02,  2.8770e-02,  ...,  1.0004e-02,\n",
       "           3.8806e-02, -1.5678e-02],\n",
       "         [-2.9646e-02, -2.2693e-02, -2.9292e-02,  ...,  1.6435e-02,\n",
       "          -5.5631e-02, -4.2607e-02],\n",
       "         [ 5.5065e-02,  1.1742e-02, -3.6392e-02,  ..., -6.2997e-03,\n",
       "          -4.1459e-02, -8.6262e-05]],\n",
       "\n",
       "        [[-2.6158e-02, -1.3868e-02,  1.0405e-02,  ...,  4.7345e-02,\n",
       "          -1.6580e-02,  2.2939e-02],\n",
       "         [ 4.4143e-02,  3.5939e-02,  1.7107e-02,  ...,  2.8415e-02,\n",
       "          -7.0289e-04,  1.5506e-02],\n",
       "         [-2.0152e-03,  1.8410e-02, -1.8286e-02,  ...,  2.6995e-03,\n",
       "          -4.6360e-02, -1.6805e-02],\n",
       "         ...,\n",
       "         [ 4.7912e-02, -3.5034e-02,  2.7562e-02,  ...,  8.8707e-03,\n",
       "           3.8796e-02, -1.5424e-02],\n",
       "         [-2.3832e-02, -1.9722e-02, -3.0533e-02,  ...,  1.5280e-02,\n",
       "          -5.5734e-02, -4.2322e-02],\n",
       "         [ 6.1481e-02,  1.4888e-02, -3.7848e-02,  ..., -7.4140e-03,\n",
       "          -4.1515e-02,  1.3696e-04]],\n",
       "\n",
       "        [[-2.5605e-02, -1.2898e-02,  8.5360e-03,  ...,  4.8914e-02,\n",
       "          -1.8547e-02,  2.7496e-02],\n",
       "         [ 4.4982e-02,  3.7080e-02,  1.5220e-02,  ...,  2.9992e-02,\n",
       "          -2.5426e-03,  2.0296e-02],\n",
       "         [-1.1877e-03,  1.9477e-02, -1.9991e-02,  ...,  4.2102e-03,\n",
       "          -4.7788e-02, -1.2230e-02],\n",
       "         ...,\n",
       "         [ 4.8534e-02, -3.4301e-02,  2.6034e-02,  ...,  1.0224e-02,\n",
       "           3.7231e-02, -1.1714e-02],\n",
       "         [-2.3393e-02, -1.9025e-02, -3.2179e-02,  ...,  1.6694e-02,\n",
       "          -5.7377e-02, -3.8607e-02],\n",
       "         [ 6.2119e-02,  1.5750e-02, -3.9669e-02,  ..., -5.9222e-03,\n",
       "          -4.3424e-02,  4.2219e-03]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cell(char3)\n",
    "cell.memory.memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/ashatya/Data/work/envs/torch/lib/python3.10/site-packages/torch/nn/functional.py:1967: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0359, -0.0117,  0.0090,  ...,  0.0460, -0.0171,  0.0249],\n",
       "         [ 0.0338,  0.0382,  0.0157,  ...,  0.0270, -0.0011,  0.0176],\n",
       "         [-0.0119,  0.0207, -0.0194,  ...,  0.0013, -0.0460, -0.0145],\n",
       "         ...,\n",
       "         [ 0.0385, -0.0327,  0.0263,  ...,  0.0076,  0.0388, -0.0134],\n",
       "         [-0.0332, -0.0174, -0.0314,  ...,  0.0139, -0.0550, -0.0401],\n",
       "         [ 0.0515,  0.0170, -0.0389,  ..., -0.0086, -0.0414,  0.0021]],\n",
       "\n",
       "        [[-0.0352, -0.0180,  0.0129,  ...,  0.0466, -0.0141,  0.0235],\n",
       "         [ 0.0346,  0.0315,  0.0197,  ...,  0.0278,  0.0017,  0.0161],\n",
       "         [-0.0104,  0.0144, -0.0158,  ...,  0.0023, -0.0439, -0.0162],\n",
       "         ...,\n",
       "         [ 0.0408, -0.0384,  0.0293,  ...,  0.0089,  0.0402, -0.0151],\n",
       "         [-0.0312, -0.0233, -0.0285,  ...,  0.0151, -0.0536, -0.0418],\n",
       "         [ 0.0528,  0.0109, -0.0354,  ..., -0.0077, -0.0392,  0.0007]],\n",
       "\n",
       "        [[-0.0271, -0.0157,  0.0114,  ...,  0.0470, -0.0142,  0.0225],\n",
       "         [ 0.0428,  0.0337,  0.0181,  ...,  0.0281,  0.0017,  0.0151],\n",
       "         [-0.0030,  0.0165, -0.0171,  ...,  0.0026, -0.0440, -0.0170],\n",
       "         ...,\n",
       "         [ 0.0470, -0.0362,  0.0282,  ...,  0.0088,  0.0403, -0.0156],\n",
       "         [-0.0246, -0.0211, -0.0296,  ...,  0.0151, -0.0537, -0.0424],\n",
       "         [ 0.0602,  0.0131, -0.0367,  ..., -0.0075, -0.0393, -0.0002]],\n",
       "\n",
       "        [[-0.0256, -0.0126,  0.0088,  ...,  0.0490, -0.0170,  0.0293],\n",
       "         [ 0.0447,  0.0372,  0.0155,  ...,  0.0302, -0.0010,  0.0223],\n",
       "         [-0.0013,  0.0196, -0.0195,  ...,  0.0046, -0.0461, -0.0102],\n",
       "         ...,\n",
       "         [ 0.0483, -0.0340,  0.0262,  ...,  0.0105,  0.0382, -0.0101],\n",
       "         [-0.0234, -0.0187, -0.0318,  ...,  0.0169, -0.0559, -0.0368],\n",
       "         [ 0.0618,  0.0159, -0.0392,  ..., -0.0056, -0.0419,  0.0060]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char4 = features_ndarray[12:16]\n",
    "char4 = torch.from_numpy(char4)\n",
    "cell(char4)\n",
    "cell.memory.memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/ashatya/Data/work/envs/torch/lib/python3.10/site-packages/torch/nn/functional.py:1967: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0360, -0.0106,  0.0087,  ...,  0.0448, -0.0171,  0.0262],\n",
       "         [ 0.0334,  0.0392,  0.0153,  ...,  0.0258, -0.0012,  0.0191],\n",
       "         [-0.0121,  0.0218, -0.0196,  ...,  0.0003, -0.0459, -0.0129],\n",
       "         ...,\n",
       "         [ 0.0381, -0.0315,  0.0260,  ...,  0.0065,  0.0385, -0.0119],\n",
       "         [-0.0332, -0.0162, -0.0315,  ...,  0.0129, -0.0549, -0.0384],\n",
       "         [ 0.0511,  0.0181, -0.0390,  ..., -0.0095, -0.0413,  0.0035]],\n",
       "\n",
       "        [[-0.0370, -0.0190,  0.0141,  ...,  0.0445, -0.0116,  0.0242],\n",
       "         [ 0.0325,  0.0303,  0.0208,  ...,  0.0258,  0.0040,  0.0168],\n",
       "         [-0.0120,  0.0135, -0.0147,  ...,  0.0007, -0.0416, -0.0154],\n",
       "         ...,\n",
       "         [ 0.0393, -0.0389,  0.0300,  ...,  0.0076,  0.0417, -0.0144],\n",
       "         [-0.0326, -0.0240, -0.0274,  ...,  0.0136, -0.0516, -0.0410],\n",
       "         [ 0.0507,  0.0099, -0.0341,  ..., -0.0094, -0.0368,  0.0014]],\n",
       "\n",
       "        [[-0.0292, -0.0184,  0.0129,  ...,  0.0452, -0.0114,  0.0238],\n",
       "         [ 0.0402,  0.0306,  0.0196,  ...,  0.0264,  0.0045,  0.0164],\n",
       "         [-0.0052,  0.0136, -0.0156,  ...,  0.0011, -0.0410, -0.0156],\n",
       "         ...,\n",
       "         [ 0.0452, -0.0381,  0.0292,  ...,  0.0077,  0.0421, -0.0146],\n",
       "         [-0.0262, -0.0231, -0.0283,  ...,  0.0139, -0.0514, -0.0412],\n",
       "         [ 0.0580,  0.0105, -0.0352,  ..., -0.0088, -0.0367,  0.0011]],\n",
       "\n",
       "        [[-0.0261, -0.0122,  0.0090,  ...,  0.0489, -0.0151,  0.0308],\n",
       "         [ 0.0437,  0.0373,  0.0156,  ...,  0.0302,  0.0010,  0.0239],\n",
       "         [-0.0020,  0.0199, -0.0192,  ...,  0.0047, -0.0439, -0.0084],\n",
       "         ...,\n",
       "         [ 0.0475, -0.0336,  0.0262,  ...,  0.0105,  0.0396, -0.0087],\n",
       "         [-0.0238, -0.0184, -0.0314,  ...,  0.0170, -0.0542, -0.0353],\n",
       "         [ 0.0609,  0.0161, -0.0388,  ..., -0.0054, -0.0400,  0.0074]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char5 = features_ndarray[16:20]\n",
    "char5 = torch.from_numpy(char5)\n",
    "cell(char5)\n",
    "cell.memory.memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/ashatya/Data/work/envs/torch/lib/python3.10/site-packages/torch/nn/functional.py:1967: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-3.6301e-02, -9.5417e-03,  8.7518e-03,  ...,  4.3111e-02,\n",
       "          -1.6758e-02,  2.7616e-02],\n",
       "         [ 3.2811e-02,  4.0054e-02,  1.5339e-02,  ...,  2.4114e-02,\n",
       "          -8.7478e-04,  2.0582e-02],\n",
       "         [-1.2500e-02,  2.2674e-02, -1.9409e-02,  ..., -1.1946e-03,\n",
       "          -4.5409e-02, -1.1335e-02],\n",
       "         ...,\n",
       "         [ 3.7556e-02, -3.0388e-02,  2.5925e-02,  ...,  5.1012e-03,\n",
       "           3.8655e-02, -1.0335e-02],\n",
       "         [-3.3543e-02, -1.5202e-02, -3.1337e-02,  ...,  1.1367e-02,\n",
       "          -5.4392e-02, -3.6735e-02],\n",
       "         [ 5.0406e-02,  1.8943e-02, -3.8737e-02,  ..., -1.0905e-02,\n",
       "          -4.0881e-02,  5.0173e-03]],\n",
       "\n",
       "        [[-3.7322e-02, -1.9469e-02,  1.5184e-02,  ...,  4.1928e-02,\n",
       "          -1.0055e-02,  2.5276e-02],\n",
       "         [ 3.1741e-02,  2.9469e-02,  2.1808e-02,  ...,  2.3306e-02,\n",
       "           5.5059e-03,  1.7925e-02],\n",
       "         [-1.2442e-02,  1.2830e-02, -1.3607e-02,  ..., -1.2705e-03,\n",
       "          -4.0098e-02, -1.4280e-02],\n",
       "         ...,\n",
       "         [ 3.8756e-02, -3.9168e-02,  3.0676e-02,  ...,  5.8617e-03,\n",
       "           4.2594e-02, -1.3513e-02],\n",
       "         [-3.2851e-02, -2.4334e-02, -2.6346e-02,  ...,  1.1737e-02,\n",
       "          -5.0150e-02, -3.9858e-02],\n",
       "         [ 4.9967e-02,  9.2430e-03, -3.2835e-02,  ..., -1.1436e-02,\n",
       "          -3.5270e-02,  2.4990e-03]],\n",
       "\n",
       "        [[-3.1406e-02, -2.0448e-02,  1.4139e-02,  ...,  4.3880e-02,\n",
       "          -8.5550e-03,  2.5010e-02],\n",
       "         [ 3.7561e-02,  2.8149e-02,  2.0874e-02,  ...,  2.5143e-02,\n",
       "           7.3835e-03,  1.7783e-02],\n",
       "         [-7.5395e-03,  1.1387e-02, -1.4148e-02,  ...,  3.0638e-05,\n",
       "          -3.8032e-02, -1.4140e-02],\n",
       "         ...,\n",
       "         [ 4.3430e-02, -3.9402e-02,  2.9987e-02,  ...,  6.9055e-03,\n",
       "           4.3830e-02, -1.3573e-02],\n",
       "         [-2.7808e-02, -2.4623e-02, -2.7180e-02,  ...,  1.3089e-02,\n",
       "          -4.9189e-02, -4.0021e-02],\n",
       "         [ 5.5659e-02,  8.6001e-03, -3.3817e-02,  ..., -9.6432e-03,\n",
       "          -3.4059e-02,  2.2626e-03]],\n",
       "\n",
       "        [[-2.6662e-02, -1.2693e-02,  8.6160e-03,  ...,  4.8148e-02,\n",
       "          -1.3378e-02,  3.2250e-02],\n",
       "         [ 4.2767e-02,  3.6527e-02,  1.5190e-02,  ...,  2.9495e-02,\n",
       "           2.7342e-03,  2.5513e-02],\n",
       "         [-2.6221e-03,  1.9215e-02, -1.9409e-02,  ...,  4.1534e-03,\n",
       "          -4.2045e-02, -6.7328e-03],\n",
       "         ...,\n",
       "         [ 4.6804e-02, -3.3878e-02,  2.5859e-02,  ...,  1.0095e-02,\n",
       "           4.0753e-02, -7.3409e-03],\n",
       "         [-2.4316e-02, -1.8789e-02, -3.1540e-02,  ...,  1.6464e-02,\n",
       "          -5.2522e-02, -3.3739e-02],\n",
       "         [ 5.9982e-02,  1.5542e-02, -3.8857e-02,  ..., -5.8489e-03,\n",
       "          -3.8317e-02,  8.9259e-03]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char6 = features_ndarray[20:24]\n",
    "char6 = torch.from_numpy(char6)\n",
    "cell(char6)\n",
    "cell.memory.memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/ashatya/Data/work/envs/torch/lib/python3.10/site-packages/torch/nn/functional.py:1967: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0366, -0.0093,  0.0089,  ...,  0.0418, -0.0168,  0.0282],\n",
       "         [ 0.0322,  0.0400,  0.0154,  ...,  0.0229, -0.0010,  0.0212],\n",
       "         [-0.0129,  0.0227, -0.0192,  ..., -0.0022, -0.0454, -0.0106],\n",
       "         ...,\n",
       "         [ 0.0369, -0.0301,  0.0259,  ...,  0.0041,  0.0384, -0.0096],\n",
       "         [-0.0339, -0.0150, -0.0311,  ...,  0.0103, -0.0543, -0.0359],\n",
       "         [ 0.0497,  0.0190, -0.0384,  ..., -0.0119, -0.0408,  0.0057]],\n",
       "\n",
       "        [[-0.0371, -0.0184,  0.0169,  ...,  0.0397, -0.0091,  0.0263],\n",
       "         [ 0.0315,  0.0303,  0.0235,  ...,  0.0212,  0.0064,  0.0190],\n",
       "         [-0.0124,  0.0136, -0.0120,  ..., -0.0030, -0.0391, -0.0132],\n",
       "         ...,\n",
       "         [ 0.0386, -0.0383,  0.0318,  ...,  0.0044,  0.0431, -0.0126],\n",
       "         [-0.0327, -0.0234, -0.0248,  ...,  0.0101, -0.0492, -0.0388],\n",
       "         [ 0.0497,  0.0101, -0.0310,  ..., -0.0132, -0.0343,  0.0035]],\n",
       "\n",
       "        [[-0.0335, -0.0218,  0.0150,  ...,  0.0424, -0.0061,  0.0259],\n",
       "         [ 0.0350,  0.0264,  0.0217,  ...,  0.0237,  0.0099,  0.0188],\n",
       "         [-0.0098,  0.0098, -0.0131,  ..., -0.0012, -0.0353, -0.0130],\n",
       "         ...,\n",
       "         [ 0.0417, -0.0403,  0.0305,  ...,  0.0061,  0.0453, -0.0128],\n",
       "         [-0.0294, -0.0256, -0.0264,  ...,  0.0122, -0.0472, -0.0391],\n",
       "         [ 0.0535,  0.0073, -0.0328,  ..., -0.0106, -0.0318,  0.0032]],\n",
       "\n",
       "        [[-0.0281, -0.0124,  0.0077,  ...,  0.0474, -0.0114,  0.0332],\n",
       "         [ 0.0409,  0.0365,  0.0141,  ...,  0.0288,  0.0048,  0.0265],\n",
       "         [-0.0041,  0.0193, -0.0202,  ...,  0.0036, -0.0399, -0.0056],\n",
       "         ...,\n",
       "         [ 0.0454, -0.0336,  0.0250,  ...,  0.0096,  0.0421, -0.0064],\n",
       "         [-0.0255, -0.0185, -0.0322,  ...,  0.0159, -0.0506, -0.0327],\n",
       "         [ 0.0583,  0.0157, -0.0395,  ..., -0.0063, -0.0363,  0.0099]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char7 = features_ndarray[24:28]\n",
    "char7 = torch.from_numpy(char7)\n",
    "cell(char7)\n",
    "cell.memory.memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/ashatya/Data/work/envs/torch/lib/python3.10/site-packages/torch/nn/functional.py:1967: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0373, -0.0101,  0.0094,  ...,  0.0414, -0.0171,  0.0282],\n",
       "         [ 0.0312,  0.0390,  0.0159,  ...,  0.0225, -0.0013,  0.0213],\n",
       "         [-0.0136,  0.0219, -0.0186,  ..., -0.0024, -0.0455, -0.0104],\n",
       "         ...,\n",
       "         [ 0.0360, -0.0307,  0.0263,  ...,  0.0038,  0.0379, -0.0094],\n",
       "         [-0.0345, -0.0157, -0.0304,  ...,  0.0100, -0.0544, -0.0356],\n",
       "         [ 0.0487,  0.0181, -0.0377,  ..., -0.0121, -0.0410,  0.0058]],\n",
       "\n",
       "        [[-0.0376, -0.0182,  0.0187,  ...,  0.0368, -0.0089,  0.0278],\n",
       "         [ 0.0306,  0.0301,  0.0252,  ...,  0.0184,  0.0064,  0.0205],\n",
       "         [-0.0130,  0.0136, -0.0102,  ..., -0.0053, -0.0388, -0.0117],\n",
       "         ...,\n",
       "         [ 0.0379, -0.0380,  0.0331,  ...,  0.0024,  0.0430, -0.0113],\n",
       "         [-0.0332, -0.0233, -0.0231,  ...,  0.0079, -0.0489, -0.0372],\n",
       "         [ 0.0488,  0.0101, -0.0291,  ..., -0.0155, -0.0340,  0.0050]],\n",
       "\n",
       "        [[-0.0349, -0.0234,  0.0157,  ...,  0.0416, -0.0043,  0.0277],\n",
       "         [ 0.0332,  0.0244,  0.0225,  ...,  0.0229,  0.0117,  0.0207],\n",
       "         [-0.0113,  0.0079, -0.0122,  ..., -0.0019, -0.0333, -0.0110],\n",
       "         ...,\n",
       "         [ 0.0405, -0.0414,  0.0310,  ...,  0.0055,  0.0465, -0.0114],\n",
       "         [-0.0304, -0.0268, -0.0256,  ...,  0.0116, -0.0456, -0.0375],\n",
       "         [ 0.0519,  0.0057, -0.0320,  ..., -0.0112, -0.0301,  0.0048]],\n",
       "\n",
       "        [[-0.0285, -0.0109,  0.0072,  ...,  0.0468, -0.0087,  0.0337],\n",
       "         [ 0.0401,  0.0378,  0.0136,  ...,  0.0283,  0.0074,  0.0271],\n",
       "         [-0.0047,  0.0206, -0.0205,  ...,  0.0032, -0.0372, -0.0049],\n",
       "         ...,\n",
       "         [ 0.0447, -0.0323,  0.0246,  ...,  0.0093,  0.0440, -0.0059],\n",
       "         [-0.0259, -0.0172, -0.0324,  ...,  0.0156, -0.0482, -0.0319],\n",
       "         [ 0.0575,  0.0169, -0.0397,  ..., -0.0066, -0.0338,  0.0105]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char8 = features_ndarray[28:32]\n",
    "char8 = torch.from_numpy(char8)\n",
    "cell(char8)\n",
    "cell.memory.memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80, 32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "controller.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_=pd.read_csv(\"data/final_80_features.csv\")\n",
    "feats = features_.columns[:-1]\n",
    "feats = np.array(feats).astype(np.int32)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training NTM for Atoms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. model\n",
    "# 2. optimizer\n",
    "# 3. loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset preparation for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.io.image import read_image\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AtomicCharsDataset(Dataset):\n",
    "    def __init__(\n",
    "        self, \n",
    "        annotations_file:pd.DataFrame,\n",
    "        img_dir:str, \n",
    "        target_transform:None\n",
    "        ) -> None:\n",
    "        self.image_labels = pd.read_csv(annotations_file).reset_index(drop=True)\n",
    "        self.img_dir = img_dir\n",
    "        self.transforms = transforms.Compose([\n",
    "            transforms.Resize(40),\n",
    "            transforms.CenterCrop(40),\n",
    "            transforms.ConvertImageDtype(torch.float),\n",
    "            # transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.target_transform = target_transform\n",
    "        self.feature_extractor_ = torch.hub.load('facebookresearch/WSL-Images', 'resnext101_32x8d_wsl')\n",
    "        self.feature_extractor = nn.Sequential(*list(self.feature_extractor_.children())[:-1]).to(device)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_labels)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img_path = os.path.join(self.img_dir, self.image_labels.iloc[index, 0])\n",
    "        image = read_image(img_path)\n",
    "        image = self.transforms(image)\n",
    "        features = self.feature_extractor(image.unsqueeze(0)).squeeze()\n",
    "        label = self.image_labels.iloc[index, 1]\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return features[feats], label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>atomic_0_0.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>atomic_1_0.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>atomic_2_0.jpg</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>atomic_4_0.jpg</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>atomic_5_0.jpg</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>atomic_32_4.jpg</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>atomic_33_4.jpg</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>atomic_34_4.jpg</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>atomic_35_4.jpg</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>atomic_36_4.jpg</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>160 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            filename  label\n",
       "0     atomic_0_0.jpg      0\n",
       "1     atomic_1_0.jpg      1\n",
       "2     atomic_2_0.jpg      2\n",
       "3     atomic_4_0.jpg      4\n",
       "4     atomic_5_0.jpg      5\n",
       "..               ...    ...\n",
       "155  atomic_32_4.jpg     32\n",
       "156  atomic_33_4.jpg     33\n",
       "157  atomic_34_4.jpg     34\n",
       "158  atomic_35_4.jpg     35\n",
       "159  atomic_36_4.jpg     36\n",
       "\n",
       "[160 rows x 2 columns]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"/media/ashatya/Data/work/self/thesis/humanlike-ocr/data/annotations_atomic_char_5iter.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data, val_data = train_test_split(data, test_size=0.3, train_size=0.7, random_state=4340, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data.to_csv(\"train.csv\", index = False)\n",
    "val_data.to_csv(\"val.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/ashatya/.cache/torch/hub/facebookresearch_WSL-Images_main\n",
      "Using cache found in /home/ashatya/.cache/torch/hub/facebookresearch_WSL-Images_main\n"
     ]
    }
   ],
   "source": [
    "img_dir = \"data/atomic_char\"\n",
    "ann_train = \"train.csv\"\n",
    "ann_val = \"val.csv\"\n",
    "\n",
    "train_dataset = AtomicCharsDataset(ann_train, img_dir, None)\n",
    "val_dataset = AtomicCharsDataset(ann_val, img_dir, None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepraring the model for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ntm.controller import LSTMController\n",
    "from ntm.memory import NTMMemory\n",
    "from ntm.head import NTMReadHead, NTMWriteHead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "controller = LSTMController(num_inputs + M*num_heads, num_outputs, num_layers)\n",
    "memory = NTMMemory(N, M)\n",
    "read_head = NTMReadHead(memory, controller_size)\n",
    "write_head = NTMWriteHead(memory, controller_size)\n",
    "\n",
    "heads = nn.ModuleList([])\n",
    "\n",
    "for i in range(num_heads):\n",
    "    heads += [\n",
    "        NTMReadHead(memory, controller_size),\n",
    "        NTMWriteHead(memory, controller_size)\n",
    "    ]\n",
    "    \n",
    "    \n",
    "ntmcell = NTM(num_inputs, num_outputs, controller, memory, heads)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(ntmcell.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch_index, prev_state ,tb_writer):\n",
    "    running_loss = 0.0\n",
    "    last_loss = 0.0\n",
    "    \n",
    "    for  i, data in enumerate(train_dataloader):\n",
    "        inputs, labels = data\n",
    "        \n",
    "        # zero gradients for every batch\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # make predictions for this batch\n",
    "        outputs, prev_state = ntmcell(inputs, prev_state)\n",
    "        \n",
    "        # compute the loss and its gradients\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        \n",
    "        # adjust the learning rate\n",
    "        optimizer.step()\n",
    "        \n",
    "        # gathering data and report\n",
    "        running_loss += loss.item()\n",
    "        if i % 10 == 9:\n",
    "            last_loss = running_loss / 10\n",
    "            print(\"batch {} loss: {}\".format(i+1, last_loss))\n",
    "            tb_x = epoch_index * len(train_dataloader) + i + 1\n",
    "            tb_writer.add_scalar(\"Loss/train\", last_loss, tb_x)\n",
    "            running_loss = 0.0\n",
    "            \n",
    "    return last_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (2x32 and 100x126)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[106], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m prev_state \u001b[39m=\u001b[39m ntmcell\u001b[39m.\u001b[39mcreate_new_state(BATCH_SIZE)\n\u001b[1;32m     15\u001b[0m ntmcell\u001b[39m.\u001b[39mtrain(\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m---> 16\u001b[0m avg_loss \u001b[39m=\u001b[39m train_one_epoch(epoch_number, prev_state, writer)\n\u001b[1;32m     18\u001b[0m ntmcell\u001b[39m.\u001b[39mtrain(\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m     20\u001b[0m vprev_state \u001b[39m=\u001b[39m ntmcell\u001b[39m.\u001b[39mcreate_new_state(BATCH_SIZE)\n",
      "Cell \u001b[0;32mIn[105], line 12\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(epoch_index, prev_state, tb_writer)\u001b[0m\n\u001b[1;32m      9\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     11\u001b[0m \u001b[39m# make predictions for this batch\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m outputs, prev_state \u001b[39m=\u001b[39m ntmcell(inputs, prev_state)\n\u001b[1;32m     14\u001b[0m \u001b[39m# compute the loss and its gradients\u001b[39;00m\n\u001b[1;32m     15\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(outputs, labels)\n",
      "File \u001b[0;32m/media/ashatya/Data/work/envs/torch/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/media/ashatya/Data/work/self/thesis/humanlike-ocr/ntm/ntm.py:59\u001b[0m, in \u001b[0;36mNTM.forward\u001b[0;34m(self, x, prev_state)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[39mfor\u001b[39;00m head, prev_head_state \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mheads, prev_heads_states):\n\u001b[1;32m     58\u001b[0m     \u001b[39mif\u001b[39;00m head\u001b[39m.\u001b[39mis_read_head():\n\u001b[0;32m---> 59\u001b[0m         r, head_state \u001b[39m=\u001b[39m head(controller_outp, prev_head_state)\n\u001b[1;32m     60\u001b[0m         reads \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m [r]\n\u001b[1;32m     61\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m/media/ashatya/Data/work/envs/torch/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/media/ashatya/Data/work/self/thesis/humanlike-ocr/ntm/head.py:87\u001b[0m, in \u001b[0;36mNTMReadHead.forward\u001b[0;34m(self, embeddings, w_prev)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, embeddings, w_prev):\n\u001b[1;32m     77\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"_summary_\u001b[39;00m\n\u001b[1;32m     78\u001b[0m \n\u001b[1;32m     79\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[39m        _type_: _description_\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 87\u001b[0m     o \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfc_read(embeddings)\n\u001b[1;32m     88\u001b[0m     k, beta, g, s, gamma \u001b[39m=\u001b[39m _split_cols(o, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mread_lengths)\n\u001b[1;32m     90\u001b[0m     \u001b[39m# Read from memory\u001b[39;00m\n",
      "File \u001b[0;32m/media/ashatya/Data/work/envs/torch/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/media/ashatya/Data/work/envs/torch/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (2x32 and 100x126)"
     ]
    }
   ],
   "source": [
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "writer = SummaryWriter(\"runs/atom_trainer_{}\".format(timestamp))\n",
    "\n",
    "epoch_number = 0\n",
    "\n",
    "EPOCHS = 5\n",
    "\n",
    "best_vloss = 1_000_000.\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(\"EPOCH {}\".format(epoch_number + 1))\n",
    "    \n",
    "    prev_state = ntmcell.create_new_state(BATCH_SIZE)\n",
    "    \n",
    "    ntmcell.train(True)\n",
    "    avg_loss = train_one_epoch(epoch_number, prev_state, writer)\n",
    "    \n",
    "    ntmcell.train(False)\n",
    "    \n",
    "    vprev_state = ntmcell.create_new_state(BATCH_SIZE)\n",
    "    running_vloss = 0.0\n",
    "    for i, vdata in enumerate(val_dataloader):\n",
    "        vinputs, vlabels = vdata\n",
    "        voutputs, vprev_state = ntmcell(vinputs, vprev_state)\n",
    "        vloss = loss_fn(voutputs, vlabels)\n",
    "        running_vloss += vloss\n",
    "    \n",
    "    avg_vloss = running_vloss / (i + 1)\n",
    "    print(\"LOSS train {} valid {}\".format(avg_loss, avg_vloss))\n",
    "    \n",
    "    writer.add_scalars(\n",
    "        \"Training vs. Validation Loss\", \n",
    "        {\"Training\": avg_loss, \"Validation\": avg_vloss},\n",
    "        epoch_number + 1\n",
    "        )\n",
    "    writer.flush()\n",
    "    \n",
    "    # track best performance and save the model's state\n",
    "    if avg_vloss < best_vloss:\n",
    "        best_vloss = avg_vloss\n",
    "        model_path = \"model_{}_{}\".format(timestamp, epoch_number)\n",
    "        torch.save(ntmcell.state_dict(), model_path)\n",
    "        \n",
    "    epoch_number += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "new() received an invalid combination of arguments - got (list, dtype=torch.dtype), but expected one of:\n * (*, torch.device device)\n      didn't match because some of the keywords were incorrect: dtype\n * (torch.Storage storage)\n * (Tensor other)\n * (tuple of ints size, *, torch.device device)\n * (object data, *, torch.device device)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[111], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m a \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mTensor([\u001b[39m2\u001b[39;49m, \u001b[39m3\u001b[39;49m], dtype\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mint64)\n",
      "\u001b[0;31mTypeError\u001b[0m: new() received an invalid combination of arguments - got (list, dtype=torch.dtype), but expected one of:\n * (*, torch.device device)\n      didn't match because some of the keywords were incorrect: dtype\n * (torch.Storage storage)\n * (Tensor other)\n * (tuple of ints size, *, torch.device device)\n * (object data, *, torch.device device)\n"
     ]
    }
   ],
   "source": [
    "a = torch.Tensor([2, 3], dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "one_hot is only applicable to index tensor.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[110], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m torch\u001b[39m.\u001b[39;49mnn\u001b[39m.\u001b[39;49mfunctional\u001b[39m.\u001b[39;49mone_hot(a, num_classes\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: one_hot is only applicable to index tensor."
     ]
    }
   ],
   "source": [
    "torch.nn.functional.one_hot(a, num_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(17)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  4,  5,  6,  7,  8, 10, 14, 15, 16, 17, 18, 19, 20, 21,\n",
       "       22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"label\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "950bd48d5df7661c49664b65d899f9df442163113fe67b13946103086fe16d50"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
